{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[255   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa4bc2a1c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAADKCAYAAABe4wDhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAB7q0lEQVR4nO39ebhtW1XeC/9a733MuXZxQCk8QQpBRXPVK8kNghUGiyQWJDyJ+QzERFGu+CkYzRMLjEnMzVUvN5/RiHpVFEMRxCKJiko0iBpzjQWixqBoBAWFIKAgxTl7zTF67+37o7Xex5j77HPO3vvstVexR9tnnLnmXHOOOeZYa729jbe97W2iqqyxxhprrHG2Ihz3AayxxhprrHHjYwX3NdZYY40zGCu4r7HGGmucwVjBfY011ljjDMYK7musscYaZzBWcF9jjTXWOINxZOAuIp8qIr8rIq8TkWcf1fusscYaa6xx15Cj0LmLSAT+B/BXgDcBrwKeqqq/fcPfbI011lhjjbvEUWXujwNep6q/r6oj8P3Ak4/ovdZYY4011rgs0hHt96HAHy3uvwl4/N09+UEPepA+8pGPPKJDuQmhClpRrVAVUPoVkYLa/1C/vxdi/xMBRBARJAQkBJC1JLLGfYs38Sbeyltv+H4fxIP4AD7ghu93jWuLV7/61X+iqg++0veOCtzvNUTkGcAzAB7xiEfwq7/6q8d1KPctVNE8ouMlyu4SddyheaKUQq2VWpRaDexrUaigCCD2n4AEQVIgpETcbEnnzjOcO0/cHiArwK9xH+Ir+Aq+kW+84ft9GA/jBbyAj+Ajbvi+17j6EJE33t33jgrc3ww8fHH/Yf5YD1V9HvA8gMc+9rGny+BGdU7AtaKlUEtBS0VrNTBvmzqw968v2xUgqoQKWu31bX+hVghyxUOQvS+u/Jw1br3IZO7gDgBezat5MS8+kvf5DX6DX+fXefjen/l+HHDAlu2RvP8a9x5HBe6vAh4tIo/CQP0pwN89ove6qaGqNIRWVbQWtGRqKZatZ99qoVa17L0qteJgD6iDsSfwQcX2BRAikjMyTUiMxDRYet9eICCIPVdsX7Ji+xpApfLtfDv/iH8EGB1YqUf2fk/jacg9JBZP42l8E9/E/bjfkR3DGncfRwLuqppF5FnATwER+F5V/a2jeK+bG41Pt+xataI5U8eRMo6UcaJOEzUXSsvgVSkO6lqde2/Zu4CgVIEQAlor0Dh342y0VvsDCmK3C14eCU7Lr+i+Bnwn38lX8BUUyk15v3tbOJ7P8/kyvmylbo4pjoxzV9WXAy8/qv0fSyhGyZRCzZMBe57I446625HHHWWaqI1vr1BRqgO711oX+1OgEoAQIMRArRVtC0it1DwgIRAkIBJ6sVViRGJCJK3YvgYAP86PMzEd92GscULi2AqqpzEUp1ZKpowjddpRRwP3PI7kcXRwr1StVBUqUJ1rn8F9ls4IlaAN4IU6ZKN0SqbmTNwMhBgJISASCQ7qIQ0EBEI8vhOyxhprnNhYwf0aQ2uh5kyddpSdg3q7HSdyzg7uSkV8c7UksKyoilZQK5wGlCAYqGs1Lr8WanFwj4kQIiFFYtzY60NAUwINyEq8r3EC4xv5Rv4N/+Yeufk1jiZWcL+nWACxAtRqhdM8UcaRvDsk70am3Y5pnChTJudMcQqmIpaby2IfutijVqQWpBaCKlEw1U3j9Ku9V0yJGBMhJmJJkNSLscGy+BC4p340E9Ssf1xnLd7Le3kH7wDgB/lBfo6fO94DukK8jJfxRt5IIKDoHsi3+w/jYYTV5uqGxwru9xA903aQL3miTFY8bTTM5FueMtOYydn5dkAJqPgts0im7V1UkVoRp3FUlKjtNfYv1moSy1gJyaWSavuKEpAYTHETE01NM4e4oEb8HivIn5F4D+/hS/gSXsgLj/tQ7jHeyTt5FI+62+8HAs/luXw0H81f4i/dxCM7+7GC+z1Flzo6RTJlyu6QMi5omGlimrKBe27grq6rqQ6mtQOsdS3JYv8g1VQzrZs1iqlmoLQaLlohVHUtfOuA9TyoVGpKrqbx92gKmxCQYJy9Slgvjs9I/Cl/euKB/WqiUnkWz+If849XcL/BsYL7PYWqKWKmHTplB/Qd+dDpmHFiGjPT5FvO5NKya0GULl3soC5uKyA4M6OIgqj4giDWxVoUlUpEXDMPoVRqqfa91vCUM2UYCA3cw/w+IUZCSoRhg8qwJu1rrHELxQrudxfNLyaP1N3honjqVMxuXAB7YcqFMXsjUzWDgaBiipYO8MGo8WAZtpE9QlVBNMzZexUCdHom1IIWNalkMd4/FS/sDhMxuYpGgu97lkrGzZYkQghxVdackVCUf8+/P+7DWOOExwrudxMKpmefJsrukHzpkhVOp4k8FqZp6qA+5cJYCrkUimfu0nciBBEXskNP2UNL3cW9Zqy8hBpNowgUdXAHDZVQlRAKWoJr7YU6jdQUCTE4uLvpmMslQZEYCXGwbtc1zkR8C99y3IdwQ8M6tHVVfd3AWMG9R9MqzsoYU8XsyIeXmA7vZDzcObdemabKVGoH91wquVRKKVCVoE634MDetZDtlzf097KarfT70gBelOh9gFWE6Im5BqhBqFH8NuyBOyFASsRhgwZB0kAcNu5Vs+Ddz+of0o2eUXACztMf8Ue8nbf3+2etWemb+CYezaN5DI/h/Xl//hx/7rgP6cjirbyVN+9bbR1JrODuYZ4v1Xh2hdqAfbezjP3wkOlw52CuTMW2vPSUcWVL20eHmFpBghdYq+G710y7GGcpu6wgYsVYddVjlGpFVQf3GIy+0SDUKoQSZlomBCuyVkVDJKQNIY1IHIgpoY0m4owpaJo/j3q32I3A+MW5Oq54Pa/nc/lcfoFfONbjOMrYyY7P5/MB+FQ+lSfyRB7No/lb/K1jPrIbFz/Cj/C7/C4/z8/z8pvQvL+CewutaMm21UIZR6bDOxgP72Q8vMS4O2TcTUylkAtMFXKB0sC9FigFakVqM/UVS8MJuNcv1IJ6aq5u/HVXn3ejarSaGia4+EUbnaMznaMIgWDtUmILA2K+8lFB44jEHYQEEtGN2xnE5DLKs6OgsfXZaDFqmc3YgMs9Gq72M4eUjOY6xvgdfudMA/vl8ZP+78k8+UyB+7/l397UWskK7i20omWkjodzxn7pTnZ3vpfdpTsZLx0yjpmpVHIVigqlYgZh2QGlVEKtZuGL6dADQhBtyOMyScu4ae6OTtE0t8e+NFQQkXnwR6fpLaMPYgVbCUKo4ouFcZcU4+qt9zV2+qfmDTENxI0SZTCAP0tRq/v+THYl1ZoC2kCUy2H9XlA+YQC/xhqnLdbfWgAUakWnHeXwTvLhnUyHl9hdusTuzkscHh4y7kbGqZCrUmqguMrFMsWClNobkkwQY6AewMBF8LS6sz9Gu0hwgwJ3gdQZ9BWTU0rrTfLX2IMB1fb6QHHANxtiNyTTeaFo5jaat6TtgS0IMZpn55kId+yspVNqmjO1g3vo9FPvkrwc513BJIt8/7iBfceOl/LSYz2GNe57/Cq/ym9zc0dIr+COASI1U8cdZXcn053vYfRsfXdpx+HhxDhmxlwpFaoGqoMrDugN2KObgAmB4F7rwtzUZFx86zKdde8i4otAWBRXxfhxXYK7UTOq9mVFsHnkgooBe6l+PN1Hfgb91uEaUiIOQ+epj5tXvq/RFrZaMnUaKbsdNU/UanCN/1QQuUdwv5yojwcHR3/w9xA7dvwAP3Csx3BckclMTAycfpXXq3k1r+W1N/U9b21wb3RHrZRpZNodMl26k/HO97K7dAe7SyOHhxO7sTBOlamogTvRuXTxKUoV6kzJiAgBU8ss3syB1GatVnAyvQIRIc7ZfcvCZzG8Pb9l+wSjerBFRBy4uhNlseaoUCtasBpAdbsDVUQCw2ZD2WyIQ0XizU7f76XSeZ2FUM2ZMk2U0TL3Ms3gLrSZtItOYdgDeGNtdA/zNd8cb/TL4528k1fxKu7kziMduHGS4+W8nO/he/givuimvu8v8ou8h/cAsGXLJ/AJvIbX8Bbect37vNlZO9wK4L6noFjc718rdZpcFXPIeOjbpUN2u4lxlxmnylggFzUqBPZsXKRlyuoSSP+++UKG7gpZqebvTnW5pQ/dwNPwflx0fp2W9Stztg/UEJDqC4Yad1ONeaH4k7UqEiq5uIhmDIQwESdzr0zZbIWDyD6e+gdYKml6Zr84nwt9z1UC8rWCut7D9y57WNUsl0c3dRsn6jT6aEOzSybcC7jLPiUD+ACVowlF+Tq+jnfxrrt870286ZbN2Fu0Zq3X8/q7fc4TeSJP4kn9/nN5Ln/IH96n930RL+qy04tc5Av5Ql7Oy2965n1f48yDe9Os93Z9tUEYtZqiRLVS8sh4553s7jQaZnc4sjucOBwzu6kwZgNIVzkiqHf5i4F5y4r339VoAq2uatEO7LUDiDctNXDsC4Z3qjZJn+9L1fTuoqaMCTGazLLx6YobllnmLkCpSpBKKWLKnlwoOVPGiTyOhJCIS124e9O0aU9tQMjyk5lrpV2B7Gs+2+HOx90LvP3r+Xl3C/X9eHSxP3tcLntKqykoSpkms4bwyVg1Z3+lnRWbXuVnWOZFGljUNpY/R7HfkyOM7+Q7+Z/8zyN9j9Mcr/R/dxc7dvw1/lqnbl7CS/gVfuWGvf97eS//in91w/Z3M+PMgzs4yGabc2pDMAq1+tzTXCjTyHjnHRy+9w4O7zhkd+fIeGh2ArupMmadgV1Ng96LppjkMCjIAiQrNoVJUSu80rj2GeAktP1JK3t21FKMTlFayt4ydFAJlm3XDDXZ84IDqu/fMnw1CwP1rL4qtVTKVAzcD0eESC1lXli8ehtiRNxq+HIpoDZdf8mW2TbQ7ZjejM3qgoryBYs2R/auMkW9DMhnvboNNVk6dNLPabDFU4VajJKp2YamaKnOZAUvlnq5VKQf6mXjbH1vslhErpMjWuOmxLfxbTyZJ/MpfMpxH8qJizMN7g0Y1UG9TFPf6jSScyZPI3ma2N15J4d3XvLMvTCOlSkrY1GyNZ12iaMoBCpRrEwXHQw6c4GP18Oy/Sp1kcuqZ+syo4s2cJ+pD2m6+J4d1xkQQ7UGppoQ13SLeNm2rxziOOhgJgZbVc0zvkwTeTciCLWkbiSJuLQyWkYfgmCSmqbDVB//Z46Y1NLNz1wCBNqy+opqsVsH/HYFYoqedrhyZWDvsiKjvVBzyO/fJrQhhead7747Wq1XQbXNpPUF1N02/ex2ugzmkiv9GczHckTxIl50RUpmjeuLH+PHeANvOO7DODFxpsEdNc60ul1v3e0o48Q07sijXb43v5jDw0MOLx1yuJsYd4VpUrIXUIsjc/vjD7iOHfcBowG7ZYRVDDgylcJcDG1UOp4bRm0csEGLu6/3LLhqnQGutuUCpIYuwaQWRBNU7ZJJaSobxJQ1spBXqlKrkqcMMppVQo5Ow/gHiYGYLJsPKRKS7dtAtTq943LDUsxLp6fCeAORTZnSmucrFqfF2kzZOdFvmfvi6kOtSA3VG8MK0kEevxoKVCKVaIvXghLq3Hlw87ZQobZFrk3JopcqvUXAf76LX6EjBPdX8kru4I4j2/+tFr/Cr/A23nbch3Fi4myDOxiwu4qij8TbHZoyZrdjPDxkN03sdiOHhyOH02R69kkp6vjCXN9sUBzRDu5LYPdEnOIce1HtADJ3k87qDcF57UYGiAGKuA7GoKpRGO3/Dl4tha21qzws+RZEZ7AWz9qt6xVzlsyFwmhXIyUgQbpjpcSAarVO1jQQU0XFvXCKTYmyKyDntVvmXnUB7i2DzsbRa/Uh4c2P3mfK+nlpNQbFF7IG7lpAC1IKosVHEzq4a6QQqSQItuRKEEKwqw8J5raJ64maBYRJTNUnZdkhh3bouv+zPipo/33/t8Z9j5/lZ/lwPpzX8JrjPpQTFWcL3C9Txhh9kH2brLllmmzIhk9QGvs0pcw4ZXKu5KyUauCucwLYQxZbf2sc2HG+XUy1oqIOYkt1hhcsu7e7dB92AVN1uB9Nh5dQUfWcUi5790ZhNHqIJY+/4PPdjKz6uMD2fqpxBndxQBQhpELME7UMnXMqJVO8flFKccqrIl0SpH3+q6qBe615BvZWGG6MCzrz54uM3fZRUc1OPWWCaTx9IRDP2pODewQJhGAmaiH5LVZ/qF78ns/f/HOj/ex0fmwhVrohcQd38Cye1Zfp1/P6W8pS4CjjOTyHV/EqXsErjvtQTlScKXBXsPZz52drKdRxtNF4U7Yt29bcHM2HvTLlSq5K9uHWxUGo7XkP0LsGsikvnIpxOqaIds69JbPSPWY8U5doW1iAvNsAi0sbEbUhHmrHJOo6GxGI5s/eCIhWzBW8uOv3Yrsm8JVHFwAvxWgjFfr3DfDt/WWarLAadsRieu+WsZc8eYHaOHcDdb8tDdCzFV01m+2CLjL2nrlrp3Msu7efX63F92MzZqmFUPMC3M1WoUqhSgWfNiUhkjQQiahGVzcxD0yJi0XP7Rvs8y9kkEdAxYyMvIgX3bKa9aOMSl2B/QpxpsC9Ffu0AU/OlHFH3TWAnyg+Ci+XYpa9RQ3U1TN1py9agXLZ/j/fOn3SaJQO6kbHFJSqvvXUzxQu4oMz2ibBOG5xv3fLGrU5DPQipGidFRxgNESIe1XcpVONdvBi7pRd8OLqhV6CmH1Cp4X8icXAfRL7cCVak1UtbYEs3bulttF/3gFbS3PIzJTSsnenl5q9QrsqmqujLlEt/aqg+CIhbsomtdEyis2ntbmzKoIERWIgBnXgV5d4qslHg9sd9+Q9EINQaTIhzMunH8+i0rrGGqcwzhC4OzvtHHttGaY3tdRpmkGjFkpVcoWMm4A1TtozPGksBfSO0wDG5y5mlBoNoxSBDL2AWhdceyNHRITQqIMQbbZp9EamBS0CCyDGPSUXHIEVSulTl4yz1sWxtnedaZngL2+qlqa7R20hCY3ZcQNLspWEUePZQ4guhGkAnGdwxwu1DvS1qj+nUrLRK1UbsM8LqIGozGBaq9FhuXYrZfPHd8690zLaUm37eblfvmXrANWvlOx81FgN6HXO2EXEGsEaLaZ+VEtwb1dQN+S38+gKs2uscaU4M+CurSBXinmLePG0UTJ5MuleyZlS6kIJI2TE3BODNSSFsJiKhGXEQcQKqIuReeoZfPVs3TanYtzNcaZzGrBHYrCxeDHGXvibZ5/i1MjsFNnY9MbAt7Lq0gulyfdkCWD0ZcUKrNKuBKywalcGFTSYIsh3Z26SBbJCteJp2GtkUlqDFg3Q/bZ08K8U36pTXXaUsWfDivPgja8vkKsNPWlXV9l8ldFaCbU4/dS5FjtYrW56XLv809Zo49qpcV78sPJqGxze6DD1BUNaMVedIrtB4P63+FsrJbPGTY0zA+6AN9dMPhpvZ3NPXdFhfLs3LRUDoVJbc1KHRs+uBfWiZst6u5ikZXvLbkfPg6t7xrTiKs5wBIIBuwQHdmsOMj4bJ8pncJeuopmPqUF1hwd1MZ9WoyqcyrHxrE0dIoRFwtgdDjxzbzI/1eqqEkx5GLDGqKqoNMrGP8+i+Nsoo+ryyuKdv9l583a+24DvXiBun9kXZOmcfaX6AJSpAXwuNgClFpdBNnsHo5RsWa42qUq8W1gCoQpSCjGEThm1zD00w7YQ7efcFps2SKW042ul1fseq559jZsdpxfcr6CMmY2jGsduGXzxDDA7V1yKZe+1lq7g6KABC9olzK3+uBJdXIUhBvKmIa+WGSN+We8gFNQxSDoVY5l7IHaeXbwCKh08JSzBPdCcDOfOSefk69xU1Rahvhi15/VXLFQgTs1UrYTaMvLq3iuYtFHss2hf9vwKIbSsdyHT1CZx9AWuuha+ZfI+FQrR+dyKXxnVpl1vypuW9Zee+ZuW3gehtMKxBLd8MEuH6OqjGgzIa61oiAtuf//XR/rZaceD11HaGtTJ+ev69fx+vp8X8+J+f5U9rnGz4z6Bu4i8AXgPlutkVX2siDwA+AHgkcAbgM9S1Xfet8O8axi1qrNnTMmU3WjqGN/yaDNPa3GJY5kv9WvJVJu20UEjtExtbtf0rFmdjoHoXLkE485VILiEL7R2e1XaNCSrk1oRNcZESomYEiElozqCW/U2OqYVXT3bB6MWlmPx1C8NAoIUrw/44mRDQhplYxA2D+Cmg5z0ImaxrF2tMWo212qFXPwqoRV6leBUTs94y0LlUlqmPhdGay4z7RHacuMvrrP0EbeG0JzR7BYH2RU5XffeFgn1xbYQJZjUEQPsKtqvoBrj02q33AXo98FbZoKLyx0iryVez+tvyii1Nda4u7gRmfsnquqfLO4/G3ilqj5HRJ7t97/qBrzPXUKrc+w5o52KaVn7rlsNNFCfSqXkOgNOH4tXuwbaLFm88acLKbTTNeJ8eYhNyogX64wiKU3NIS1z115EjQ7sbXMOxRuf5gJg6BOc2gJD591NYjnTJCHQx7K2emxvn/dM1K5yln6HDbhbQdOtDgg0h8n5ybNyRHEb4UXeS9M0Oi/eumabakbb5scDxY9NehG10S7L51PyvFWlqnlAtKUKbGGPITjwB6/NNhdNmSkyHOCr7mfyrWmqnShty45/rs7SX+svpt/eGEZnjTWuK46Clnky8ET/+oXAz3Fk4O7KmJ6xG89eR7MZKNNInrIBe63k7EU+zwbnQRttYpJ3hvas1/1ampmWBAd248tDiDYuT93HXW1YhzYxd1WI2jP3kOKcucfYs/Yq9r5tMWnj8xz7O4w2WA2NeQ+eSTuIBD9iy/73LXnV96MowbXzdoExX6V0mY2avHCZ5fb8X9WKseqmZOYl4HNLF+6brZGpNDdOuoKlFyr7eXJevWQHdrPgbBx7o7maEZtx55a993MtvcxMUz3NpmLMSp1+xcLdZPGX8+zXjtC/w+/wPXzPNb9ujTVuZNxXcFfgP4n91X6Xqj4PuF1Vm6v9HwO338f3uJt3tmyxjovi6e7QNp+Bmt3cKhcr8k3O4ebsWWV2YFd12jt48TR4EtcsBYyq6YZaMRHSYLx7EJf5KZHWQOUujZ3LNx49ppa9R0J0SsZBqgF3q60275olqDdyRRvhsBCNdNpkD6Ac8HyGa3VQrdoWM1xj6em/eyfogmfuUOeczsyz+9lx2+QG8B3Yy/K+g3QIfgnSrhi0Lwgt86f75VihuBdR94jz2S9nGXYuZjVTk4rOxW+7b5/pygAuy67gdoKvMd4j71kNrNY49riv4P7xqvpmEXk/4BUi8jvLb6qqypyu7YWIPAN4BsAjHvGIa3pTG6e20LOPI2V3OIP8eFnTUp27T0tRa5vPM99uTT5CdG69iRDVgUBdISKXFUVDiF1X3ouKMhdo2y1O6YRohVQrrtqCUVu2zj4RsATs5fdqpxIM4GVBuCwLnzrDMoo18rRXNt18tyhuybGE+ZiX57v9zGgZb+0/h9kQbJmxL+9Xz+4VtFEyjf7Cgb9d7cxAbg6c0geJmC+PUOyXxwvai76B/phJHFsfgTh9FlqdxIvbNEVU+9kivQeg1SPsra5tgHil8nyef02vWWONo4j7BO6q+ma/fZuI/DDwOOCtIvIQVX2LiDwErmzT5ln+8wAe+9jHLhiEpQpm/iPzb7bXopNRMb2AujNapg1qyNPElG3iUAP3UswzplYH9tLUF17ApHZKpKGrItTm+yJCFCEGuw2tpZ0GvqFzy1qrUzoOvEFmumX5obSRJTNXLvtlveUn9/uNrmhbe8JMxczw37L96stE+57RKzVgYwKlce/mjd7ftzX/tB+FtuYnEK1uK7AE89ax6t/zovf8cdscWdn/DMslxJUvhrv+vbYa+M8khNjlpa1/IASXmobgfQSJEBJhsAJ2TANhGGzodYpo9OlM/l79rRa/d7I0FQKewlPucTIQwK/xa/f4/TXWuBlx3eAuIheAoKrv8a//KvAvgJcBnws8x29/9Fr2q9ALco3HnUFee7ZXp5FyaNl63R1SxkOjYsadGYFNppSZSnFwh1xnmZxUdXB3MA1YZqdKlPlYVBbt/FgjU8QUKVFaJxBuQeCFS+esdf5ETmk3n3Yxoy1X4kBbA+Yu08sVeHMOTgdEdZVPoz32uiBDA/XFgsOsm9FG01SoTdMv4jUEB/jG7LR9O7+t4LNJ7XyWaoXk4l/XqiZhrKZ/r76P4IVjRdFFR5aBfei9ASFE6z2QQJJA0d7TSvXLGhHzjzFZaSKGSIyD0V5OnaU4kDYDw2ZD2m5Jmw1xSDYzNkX35xH6gPK6D+zg064W8Rpew2/xW9fyK73GGscS9yVzvx34YZfoJeD7VPUnReRVwA+KyNOBNwKfdfW7bIBhmvWas3GxugR5u3yv02SF1F5ANSrGPNoXmXvxrF0XPuJe7GwzSFs3Z3BtdyvWtSyxZeBB1N3DK0lbg5Dl3EYlz4SJop3rVumONdZkA/Tq6Yz9e7d3EWrInL1rz9a1jwvcy34b5YHRBFXEh1rQVTlL0y4bdYFlx9XfVAzk5x+N9reoaueuKVBKxbx5XNdeXNdeqjU4tcy9Kl7oVbfWFT+EMHNRXeNfiVUJofYFqTk84lJRu5Lyjt8QSTER40BKiZQG0jAwDAbsw8EBabshbgaXoAY0utWxv//S8qHFEtx/hB/hj/nje/slXmONExHXDe6q+vvAY67w+J8Cn3x9O8UzUrMQqONomueae2FN+/dN1250zFLbbts4jUzNakBnb3b192Ax99Q6Qo2aCJ1/NWCTUH3wheXAESVpISmm9/Z83PfgnDgOYP6Vc9StZtl4+J5S47dde6mdiO+LhTTixheMxnU3X+JWqNx7LmgQv6Koe0XIRq0A3rTkBmPOoBjoLVaY/rNxkK/zQlnUrRwc0Itn7PZ99ij87o/fGa2mlxeIIMF+NnFRt4j2diSZwb3ZOQgzFWOgbtuQNgzDhs12y+bggM35cwwHG+Iw9Fmq2ruCZVE03l9TQ5rB/ef5ef6UP72qX+U11jjuOFEdqkbJVGtgmSbK7pA6TZAnV1G4H7g3IdmQZ/eNcV92u52YpqmDe1Eb7lD9TVrBzvBN6T36QVANPQteZtMt9w24uqa2Jpo2EWjOdBvlERzYeymzgX1DkoXtgT3YMlh7TnXQbW687Sqigmm/G3h65q7zwdpxiJ/PsD8M2nC6gSdemKzdWiBoQercGas0qmb+QbUhUb18oa0+ra0vaTHpaIZLdcqnHWg37modumoOj/MvhF+ItFO0sGRwY4de6B5capqGxOB0zGazZXPugMG3OAz7FWukH570K5V5NYrpRP2JrLHGVceJ+821LkdTwTS6RfMEucxKDO9kzLmYKVjOBubjaLfTZPr2nPeGbjS4nud9ugIFo1y6+gPTrDc+3VaEgOnZHWmrZcczMb2YWQoz2DZ1dqdN6gLoA51o7poY0ODZd+9cxTn9uRGpOqjXDu7tQqDRSSzmxdX5TqeCGjWCL0IG+NK6Txs/LyCL992r4+os5++CmD64uhVG54mk0kG5FUyN7zbeO3pRcy7mLswBZjCWZgrWFh/zxg8xklIkxbgH7sN2Q9puSAdb0sHWwPqyesYc7TgXlxrXaT+wxhrHHScG3NUHLWvJxrc7f152u569q3uWWGu6+bKbb0xmmgzgx3FknCamKdvgjTr7tLdMLSx5AhzUGv/ekModAptMrmfA1btEg3haOmfu7erAWQuceWehDgdt4A5tUTCDAsCLiqGqSSTb1A0//pZYLhSMrrFf5Jrte2rFUqIhvUh7ryuc+2WirFd8ymUvmF+ni4+iDtbSu3OjDceQpSZfCOISUgd1YnudQbk1Y7U6iHPzDuxNySTRB564u+MeuKfEMCTSdiBtNqRhQxqs2CrxytJGuZt738y/5rk8F4B38I6rODlrrHHtsWXLP+ef83f4O9f0ug/kA+/2eycD3FWteFrVfdi9+WicyLvR+PfJR7tly9pLaeCeKbkyZcvUx2liGk0lU0q1rLJRC/43e7nBVnR0amPeogYrVEpwTHFvGQJVKlGFWudOVnUz19oy10ajdFpd+/73SeiWec5FW0FoTaMtWW2wvGBFFly8eansAby/QsBdEo0C2u84WOSngQ6u0rTinRS/Ahgu+KrmWCmhjbSjnxeRYJ2nl+nWRebBJbrQnHeb4sapM3fpWtbvZywE069H9+hJg3X/xkB0K+U0RCuotiJqir4gXFsm/k7euTYkrXHk8Ql8Al/FV7HfXHff4kSAu6qButbSnRzzOHUP9mnnNr6lzHTLnsOjUTSTZ/JTt/V1nXe7tveCZaUVSBst404B7uUem9ui1A4qTUkTHfNM596Ov1EULoXs7ylOqTix0YHdwVZsBLZlurYv6bzy7Oi4BNF2v5M8YhOgOo+OfUNmk0ec4dmDaWvMYn7TPs/Vl7zW6cmCr69CnyTtx0QwKiUsagcaAiG4E2SwIw1YwbZbEvvioV7Y1Pb5pGXtrS9gvraxtzNwjxvXrG9M4hiGRAhizWUxkGIkDom4aRn7vqRxjTVOWtxIYIcTAu5opU47y8xH82DPUyuQGtWSp2wZ+ZQd4NvgjTzbw9bZp9301c1jpGXuLQdsqnUrema17D2qzX6I1F4sFRXvXl/QBNKIlIXfSlOStM8UZjXG7OhowGgqHdfWIwT3iFlcXPSFoQlcenNVWzwADf45F2C/rCVQITqVLxi4tjepDpbaPqfTG+INQI3b1raS+JVFw/DeuKkmS2yHHIj9XMwHOjtWBqxXoJ2TKtKLpdrP05ylx3YFovRhKTFG0mYgbrek8+dIBwcO7tb52ztRo/n5hCHepRlpjTXOepwIcFdVyrhDS+3SxuyAPk4T45gZx3Zr3Po+uOc+3q3zwCLzrdslzv2fFu22tbVHzDo3qhLKotYHdEMuFiDcuZCFcVZLa9tsVHH/8/Z+ujD4UgN27Vn1XHrsi8QSZFlk7zAP5Ra6iqZpx1uCrQ2T29avYFpGvihs+uDuVtxsn7PXCJpssNV+A97hG6ze7Jl4rxd7yt+uGpabXPYZ+tPbZ+7Pa3Ngm51x8KLpQDrYMpw/x3D+HGEzGIffFgYRn3IVfMDKjc6L1ljjZMeJAHeclqm5uJPj5KPxMnkqjFNmNxmw78aRcWyZ+2TA7h7ilvxKR2XVAMHkgvNfdpMiinPhMoNSy/DVPNSDuny8NlB0MeLci+9Ee7O6dX2gZ8JogBDNa70Blr/EWuZb8baiNXRu3I+SfpXRrWhnJqVRSVXZK+J2Bor9Imv7DOLCmXmP0Do+pQE8TeXSXjzvqIF+U9G0q4pGr/TRUg3Y/Yg7sCvEdsWD7af7r/dVt6lkZF4Mql9dSSClaGqY7YbhYMtwfkvcDIsrs7af+QphVb2scVwR+l/tleMb+AaewBNu+PueKHAvLm0s09Q59GnybczsxomdZ/EN3G1QszsKKk4vKKENk1AHcW0FSyt6slCPaFsQWlExhJn7lga0Tr/gCXvL0quClu5m2BqUbPRd9OHTrSArVIxeaRYAYBRGdEUJdbYuUM96u3xwzv/t+36lMjdOWdwFxnol1j9zU0Yun7j39aIArcsd7D2lv7DbFoS4GPYh80LBvLCZ2+XcQ6ACNczUTNv58hP3RUGt1pFiMjuB7YbBFTGS7p5Tl/0DXmONmxr/jf/GQ3no3X7/Nm4jHQEUnwhwV9VZBZNN/ZJz8VvbxlyZpuqSR5M/1mxZu9bcuebY7AN6P1AjdGkpMyoLZPPL9+jFuNScHx3cWwNSy2JtVqp61qy0jh3xrF2qA2EMnSJpmbo5T7a3NuCuYpONdHGJoIjJ6GvwgUezesSNAubM3c9ft+bak3lKL37aPI65smpydudWnEdZrgEdfIE2LPoKGN+Bs40MlJ69+5UArXZBX+g6QebUVYzBigNh/pnATMv0BdLPY4qRdLAhbjaEYbguFcwaa9ysuD/3531535v+vicE3LEh1m1Skm85KzkrU1YH+eK3uQ/caNOUOistECs+U9PAvTctiYIGN6+K3cq3uQmmmEghEBtH29cEy9JLtSYmVaG6UZYhbEXKbFXrn6pvjcoJbVZqy0v9kIyzn/Nu9UyWriIB0UChTWeygmibytToDXu1OFe+kGrqbBIm5ljWi5fardytM7c1cdk3Zf4oCN3mYAH+i2S7c9xL5U0HeDBZpg8db1dIYUhW8BxMg+5Vjf4ZWsZvUkpXy8RoKpjNYCqYFdjXOKERuyTg5seJAHdQlzUWu83FgL0oU1FKA/c+ONk17MWGPIga1AURU7sI3djLAN69bTHDKDS6SmOmYVKMbFIkBdNJNx/xqkr2wc8axL1R5o5OdcATNyILOtsLqANpoKO4DfeQBmLSAXWuQmLaRhE/xkbjtKxdnFkRL0T6rcwLUhtY0RQniphVgK8mol5kbryLYHUDragGB/h21bIojvZ22f0sPizeMwRbMGUB8PNVUnsvp65SJA4D8WDocsbLM3ChnzrL4AW39DUr37trSlpjjZMQ38q38v68/7G894kAd1UboFGbnLHoYoNclFrw+7X7hbdhG7h/SwjG50aUqNWGb+hcmDQuwu1k3fDLACmQQmQTEkNKpGgdlBUzwRIplOoA2SeYSlfmtC86HDYw9Apno00MSlvmbnSIcecVpbilgXYXR+2gPvPuTYNeRVxpIrNmvdnXMnd5Ls/xnmMCnR2yBaZWKz43GWOwJy8pd9kDeWZ6BXpTUnDlTadnpH1Wup7HlDYG7mGTGLZbhq0VRa9Er/Srg37h46Zh0ZRIa6xxUmPL9l4LqkcVJwLcwYqDtegM8MvhGor7hDdgbx7mi8k9qHdjWnJuRbjgxgAN3B1YNBiPLnSp3BACQ4xsooG7cd9K7m2dNp6utPZ6aXuVxb+58Wifnl6UBntG2yiPNomo0IdwMC8+unwPX3B6Vt9F9w1sQ8/WO9AuDqZn4F0GpIgGWyB9CpNWn+m6V0jVvS/tvfy2ySNxLX5ff2TRyNUAPfTXShBCskYj6yTdkjbDVTAsi1qJLO6vscYRxkY3VvS8hl+3T9RP5DP4jGP7FT0Z4K70JqTqWXrzAzfmxQyyWHDcjRqYs0dPS/Hss7hhl0jPemcb30jLmgUxeV2IDNG22DN3QKpZfYlQ2hVBrRQ8W7U9GpiqESiOXsxTfpreWnpnawcx9SNpo+l6l2u7be1SweSZvu+uLGmac52z6M5pd3Cfz027u3z/9kXvcAX/un26uwv/viw+UFe6tCzbnxXcDTLMTVNxGIiD2fDGTdqz173aEOAqVoQ11rhP8Tyex1N56jW9JhCORAVztXEiwN2wyfToTYXSbrUZbS3gTpxuacXLuTHGpYG1UkJwiaHDvgT3KgkuX1y0sjstk3zwQ/JOzeLvGJ1QSBqoIRJDtec0qqMdh9TGZhhd4LRBiG2e59zUZHYGxa2H3fvdr0Sq9k8J7sPe2JAF5tri5ioSesGRPmA7iOw/3/cxg+5M9agXntsgDgmtGLz/+rs80PXw7Q08y5f283ETsXZJ5TRYaHz74APDfR7tGmucxIgS2bC5thcd86/ziQB3wHlw7VR1xQEeH3ah1YHMhjcYYM88rFnFWvaZHeADuMLEuXgNKJUY1ItzQiT0aT4xJlJIPXO3omggFdeei5J0Pp7gWTYCNmh7VoIE9zgRv8UXDMu63QWyO4u1wmxTqzTafqaEkKa7mfXus3GZhWnIF/S7nyPLmBWqzMDesu3OiRvA2xTAuVbRCrBtMlW/AnCfnnZl1BZo2iJV7SCEOls4RCHEhKRk4/AG814Pq+JljRMcT9In8el8+rGD9bXGyQB3heoqjdZtaSDvcC/OpTvACHMGKs7n2vg2V+o1y16dvUyiCEnNN4aoJLUFIUggiatkfFxbDD5b00G3oEQplr87eAWtFFfGmGwxOEvui0YIhORTgqI7H0owF8dGu5R5oMWMiw3cpf8y9c+LdHObBs7dMkWERAN4mb1xfAeKzO2tYQnsDdzb1YItmLWax729vgF9+4FJF8701/nx16qzOkiLN4RZiTeG4GZe7tQYo7k6xrhi+xonNh7AA3hfHnDch3HNcTLAHZjBRRZZ6zJ31T0Tw5590jQqYmCrSoa9QRQRGFpRT5TYVC6486Bb0LatWd+qe7qHaOZYQQ2gaPx5DH1gc6tWtn2kEEjJhjcHB9MazMOmqNUQSgPIfoWi82zTBvHzh2VPP+6p+ZLX7pm7P94ye1zL3pG+KWxksV+cE1c/eWFPGLPA9bls3Aqm3ZzNSyJUtclOoe3P1DsxCCk1jfqmX9msipc1TnQs85pTFCcD3P3yviHVDG3QumYsI8ZVGDMwLfXi7dlFleKvb/M37Zs2Hq86/SEO8G2J2G82uGzMnjQPFZzFCIRaqRIcCGf+v3W6brz7MgYrgBah1wksE7dPaRSUT1VaNEGJ3y4pGZXWKMSM+X5e+uJE//b8CYLS0+0F/72/bNKFMW3YNwvOXvyoRObb5c+sTZRttHtrzGqGayEIMZnpl8keAzP3v8YaJyME4XZuB+BB+iCew3NOJbqfDHBnVrTsh4NDo2YwgO/AP2N6f6woZMWnL82bd7p3wU3fe28gcjli453pLTyuYnGp4vKYMKqoPdL/Lz5oe6GMEZkXGcvQqw+WXtgUa+tUbWDYMvf589IAXmSB4Lrg0C8/s9qvDpZ+Ov0MywK029P2S6QLgPerpD0ZzP776rzDzsO3n1FXDSWbnLR/nKfwr2eNMxnvw/vwJt40W1mf0t/NEwLuLLoqZQ+jmtQxdFALNuJOfcoPljWbFn5W2uSG6r6vpZJStWXv1gxVS6VWG7pdjeABdQoFpWoxBU4190nbLFetiw5TbYWAqlSpaBT/Xu3vXWol+zaVQi72dWvias1QXdWDcd/4ZyBYdl6D9EWjj+C7C8Dr3k1H3aYRX/zONtaqPcvKG9pNxrS5SS54fPznwXL30P1vAq2WYEjfPGvmfZzOP5o1znZ8lX6VNcmdVj7G4+SAu5tOdc4bu8RvMu5GJagEgkRqkwg20FT1THjO3JfZdPVCpm22CFjGnIk1ULJNQg0aUDV/8gJkIGsl+/NrcU+b2jZdZKe27xgCRYRS6PWAijIpjLWyK5WpzqMBcym9QWseR2f7aZXK7s9Oc2A0SqO2RLn5qfczaq9rfbN73Ep3fp9BXv0Uzxz7TAtpB3rpMhybZdpqACzftb2qW9GYs/HiPF12YbDGGicpHsNj9ujI0xonAtwNtw20QjBgCzOO0NrpW5WwNN7ZxStlAe4mg2SemwrdJ6a382srahq45yyIVALZFDTVOj2LQFZhUiU7lVKreuZu1ge1ul+8g2/ATMUMOE0hH9UaoMYKOwf3sRi4m49ORovZFjeFTwqYyZbLJpsFgfmfSy/SNmpGBfO+aZx748z7b6mlzaLNs72dESdlGuXTMnt/bdPhd4sDnd2Cl3LK+RrKwi867Nhru2JagDyn/m9njTMaf1f+Lm/n7URO92jGEwHuYGBVl9k7nj3qHpsNEggSOnjPWbg6LTMDe+/ZFH/MOzvn1xRCKc6LF4Kaf0ys9l5FhOwUT8YWBK1WANVSe7t+lzH6scYazLQMK2JWvwqYKoylMla3MHajtOweOaqVoCZprO51FhwMBfaAvTlaNoBXPHPvNQntnajavwCk9vM5A/uc1UvjZKAXNLRxKrSBIn6l1TTu0vfkr7MyccU18uBjD/089artUfwurbHGGnBiwJ0O6jNezYXLWT+zoGiYC6gta++cOk0BsiCWPcN0Fbbz9JVcM1L8vVS6xw0iFIRJIWMgbxr6WbbYumrt60ZaC9oHazv94T09U4WpVNuq9oHe1VPb9gkbdeQHTi8ciHRQbSdKXUaonaLRDux77IcuQHxRap7P795PY3G+dQZ5X8HaWW0mYXZ4S6RW44nMCWd+F6WftxXd11jjaONkgLuwh8MWTgdoa2VasLl+WW8TiLS5CczA781JYHRP21qxFpkVK9TaM2yjH6q5AmNgmTV45j53YdqtHU3PRh3cBXNrLCLkTM9yq0Ju5mh1Luiiehm0OhG1PBd7xeYlwM82v+2ly2Ncjt1r3wuXvdOVvl78BO6VH2+FUVkuES59pLYdXLb3lXNfY40jj5MB7i0WlEL1/5sG3PjoNqe0ywd9YEZFu/TRjMCiA18gBUhBiAIhSu+YVKH7ueQ6N05VKQTXSlrDUbCrgwV47WfEcxVS3RMGp36K57MtczfXAgO9UJVQ/UPjBVLU7BAI3lhljpX9drG1yUf7hVL1/8zBsq+Zl2GrMBsOL1vG2pXH3lVSe0VbIS4D6nZ91XT2c3dAnVeH1ujUD2bN2NdY46jjXsFdRL4XeBLwNlX9CH/sAcAPAI8E3gB8lqq+U+y6/1uATwfuBJ6mqr929YfjQOODI4pWqpYuQaTYVkohl9IlhaVl0yIdFCVE82kXA/cU1EC+j4JrBpMmNWzqmSClEzfVM/KZx+8nxf1WetmRZtFrtEwDd2yQBw1wcX8cs0SQpuCR6DSG2SREhETolggpztYIIUb/fGFu5lqk2JaxN8ilFzWXZziI0ep2zphRfLmrBuayeH3n92eirF80uPslsjAri9CJ/7bBiu1rrHET4mr6vl8AfOpljz0beKWqPhp4pd8H+DTg0b49A/iOqz+URrdoz9wbsFuGXsg1k0sm1zxPY9KZlmmTgIYY2KTINkW2Q2KT7P7gHaNNmm0Zv7rm3BQsu1zYTYXDqTBOmXGamKaJMk3UaUKnjE4T5OxbQUpFammpOaoma5xnwRYfC2gzVqMqg8IAbCSwCZGhWw4nhpBI0QzMDOCty3VYbEl83isNN2fWe1YQLfT9MEtBVbo5W68d9BpCo53mqsfS9mD5KzNz5+3c21VFjM0J020SQrM/XgD8GmuscaRxr5m7qv68iDzysoefDDzRv34h8HPAV/njL1L7q/8lEXkfEXmIqr7lag+ogUzv4NS5eahpy0tZgL4/D9wkzIduxOgZr0AMlp0347FuoYs1Fc2MvY/s69rGxqfMyWYbdK2qC7WKOg3ErL2HxUIFXYoiwRJaWdgZiFjiLrb/oOYxH8XNtsIC5J2i0TBPaGqNWa6pmZtuG0ni2f1eo5J/pirWT9C+oV1Vo4vUXLoPe1fjtMYkZhdKa7BqJmTug8/s396HlKwAv8YaRx7Xy7nfvgDsPwY3YoCHAn+0eN6b/LF7AXfd+1cv+7pp0m0CU8vk7X7tXiyGGVGEFA3gUwzEYBYBIvMIjNlSuHrWP1sMiC4GbqvxxkGXhcnq3HJEqgNVdHD1RqL2ifDjFleTmJe7GmXUDtiXjUaxNFopQs/YDdSFJMH7AGb5YdHm+o4XoJf69rnwC74G+WcPqKlr1O61VyzLu61D73IsnimZ5fvJ7Jffjk98f0HMJCx6EXjlZdZY48jjPhdUVVVFRO/9mfshIs/AqBse+pCH2L7aPllY4S626oC5pG/6/pitdlMQhmDOjObrVRd79tvloOtaqRi/Ty2OiEahCHT9+txIFRAqNfiUpFrREFAtvaDYOld9orYPBjFQDYrvA5vmjXRf+nngxuUKmdCLlks3zAbWc2FUL2tgms9QW9gEmWkYrwW0hUadlO8A3CaE9ILq/DOo2nzfF2+1OGZb+9RH6rm1b+tMW2ONNY40rhfc39roFhF5CPA2f/zNwMMXz3uYP3aXUNXnAc8DeMxHfHhzZ1kA+jyNSLHmnZnhnaOBroh4disMEryI2mSCDcwct9sXbqTesnYz7iqdS54zXeuQ7YZZ1RkG7UQIPuWC5lBpYwLNu6aZ5AaNpoQJzVrB9tykjYEwd+gSO8j3SVMdYPd1K90HxsG2OqV0mfilq3psKXP9vYj7rxtlZDbHRvvYouPcu9qCpuptqmoTm4y+2ZeqNrfHKNFtJdzHPSXzyl+pmTXWOPK4XnB/GfC5wHP89kcXjz9LRL4feDzwrqvh2xUMWBfZZINtoysiSO6g0Jxr26CKpgBJAknEgd0AHqw4WJQ+WLtl1KJKcBsBRaEWK4Yu2uP71KSWLUtfThazUBv/7LfafGvcYKz6/KRQiUSimuQxqHvEqyt4zB4NCYnmKd9pkSsITQTcQrh2mwBoE6L8M2nzW/ccX6Vn7nU5hEOCNV3ZqkV7Y1Ux+imo7cMnLNnnFKoGu8LxhcwvlWxsXhJ3gDQnyJQG+zqswL7GGkcdVyOFfClWPH2QiLwJ+FoM1H9QRJ4OvBH4LH/6yzEZ5OswKeTnXe2BNCqmF+x8HJ0B+0KpsWjkCTrTB0EaFWPZe/JMXtU08FRv6S86a+ZVCW7+ZZm38/jM6hucCqmy6HhdLDJLtroBfG1qmWqKHpsMJQiRVHyyE6Z2ieqyyirmSawzsNqVgd7lPDVWH3UKRr0+4FSS0UP2OgNwGy+I2mIxF63Vi6rSgV1ELXtfrCq929eFMqp27hstoz78Vqui0Rn4IDZCzzP24AqadaTeGmvcnLgatczdjfz+5Cs8V4FnXs+BzNy6OKg3YI8zwMeIlAihEmpAQ4UqM9+OkLyoGmWmVIwX9+7QYhm6Iz2ilVCrD9yuvW2/DcYw76DgwLgkQxrj4f43rQ7QFDylUkomF1PzWOZeKESiJmIMaDX7YvFFKmgwcGzqkiv/PHqVVBuwanGaxL42+qST4FQqgWBAbxyLv647+jrhbzSLBKXp7lHx58b+ZLNp8CsLv+KqPuNWnZ+XIMSU5sEcwaSS4lOv1ljjpMb9uf+ZKPqfmA5VCQGJtoUYISRCSHZ5H5NtpaIxIlpQNdZ6mVvGYMC+dJSEmWI3twF1AXhP5btpl+Gec/uO4+a26GDv3+uj8NRbl1Q7oGqtFC3kXJhKJjv3boAdiVopsZJqRIMw+JVBCEZx2MCkiAbXzS8Kpdq+buDeqBDX11N9/pS2litfVCQ6RWPl1FaQ7sXpXktQu0Cq2lUuPkfQf0jzgkEIcyG5KhrUj8WeE1whY0OwB6ed6I1fp/9PZ42zEA/gAXwSn7T32L/kX67gfsNCTE0RFUKsSMymrIgRSanfJ0W0RtDoGWyTLNJHzLXo5b2Wieu84fy7DTKtfQJSL9g2rbfXDqUhehtaIUatiFS391XPnufBH7kUcslMVbsOX0KkpEisCY3FrlDCTDNRfbR1SGjURfbd6hKXFTE7vdTsgi1r7x9SsA9Rq6fnYT4pqNcabL/BrxRqrfa1qn9mz9JRd0gQVANxjy7Syzb/mYbgdExYmJatscbJiQ/mg/khfui4D+NI4kSAu4iQNhskRLJCmipxmAgpITESUoSSPDttmaz/v+CNR57NYtLA7Hx1wYd3YC6PVT0D7zS7FVBbN2dLyntvkprtbtCmEMGKlbU4j+2Zv18B1GoNVrnMmXttPHoMhBKpMVFDRJ1/booSovEeEopRRbCXZdd++eGfsla0TFAz6udAfA5Sv2oJyzoGzuW3MzjXTrUdI+0Uu6iz2KImBOPaEYLUvYWnn3jm1bP3t8qV6aU11ljjaOOEgHtgc3DO2vQJjJMSdxNxGAmTg3tNdFuqYGxBlyB6f72KjcWTRYbbxu8VbT4u5gFT3TNGq3QLg1bQbTryAES1LWCZrEqTTWYfv2fj+dTH5GnJppQpmewNV11HHiMxJWrM1JjQEKjebaohUlNCiUgycI8d4BtmNrUPgAN7yVAnzAt+YXbsoKrtCgQBMXqojeigUfAe2hc1nR0dXUop6pm7BAg68/R9JdHFQnQF8F9jjTVuapwIcA8hsDk4oJZKUWEYC8NmR9gk4pSoOfWGojYYOuCzTTXiGhVUbO5pLzq6tn32VbGiYvNYKR3gW2brJLu4lQEQUSI2RANMAaM1U+tELplSMyVnai7UXCgl28i8ksneHAV0cK8pEaIBvMY0g3tM1AoaMiEXYq2kRcepzXud6wGqGS0ZyoiWCdHSi6NKcKsFQ14nr9x50mOPJVkoYnTpJY8jvjaRJgSjobTvY6HRFGbKx168gvwaaxxTnAhwlyBsDs6ZfFDhcMqkSxuGzcA0DjYIW7zo6rNDCxjHrNVdTOZCZ2m0he3dwFswEFXL3guB3PTvbo0ormMP2kbVObir2fNWLUjNaJnIZfT5pxN5nGyi0pRtxqr731iRtvrVgKl9goN7SYkai0+gitRUKRUIkTQUhlLc8119NJ3VCUozZ1BbZDRPaBkRtUlSAfOl0RBdzdIKwQ7Zfv5axXguHM3Olo1hac1JInYcAUFq7fNcgdb7NMvl+6Khe4vsKn883rj4HnjIPXScHB7AHz3i5h3PGkcfJwPcJTAMG1SVnCvDZmKz3TBsNww5G00QgFAQHzEnCqXm3ngkmj1L9zb7vQyyaWroAFZ9gcjeXt9gLi6EIY3e6e4KmqllouSRnEemaWTKE9OYyVMmj+ZWWUtrhqpOhSzAfSiEWAi5UGIhxkAJ0bJ9tcLyJrvjpTdcLX12+oCSTgdNzrsXv9owYI8aCKEVVltV2e8yWwQ0gG+Z+xLc+6JQqzceKSEssnHX+4t74vQh561IfBN+d446Hsfj+CK+iD/kD/kJfuK4D+e6YnsI/+ofwTO+++6f84cPh8/6Qfjlj755x7XG0caJAHeAEE0BE4eBYTMwbDdstltqLkwKNQitS7WCdUSWRC2pNyJJbUXBNtPTomWUKhhfHxrY7/uSB1wNE7BHu4LGMnCjYCZynsjjyDRNjHlk7OBejKLxBcc037W7R0qsSK3EWAmxElMlxEiKbbBFIE0bcilmXdAzX/qVhMicDNtEp2qLSS1EvHuX4Fp1PxM+UERYZuv+ibsVgG/d+Mxj8V6I9kI0e/guSJTepBRi6F44px3hn+T/3spb+QK+gB/jx477kK4uFEKFv/9i+P/8EHz6y+/56Y/4I3jR58Df/nfw3/9XTv3PbY0TBO4tYogMw8B2e8C585kgwhSj0xgjNRivnmsllA2huAywCoJlr7MXuUXXvAdXy4hQmnSyd2E6cIl2WWRFnTc3uWEu2cB8GtmNO8ZxYpwmxsnBfXLf9urySuqMyMHsB6QoIdqWihJjocZoGnMRhjxRSt7TjPdaQ/NEV7MKywq5qGf5xaY4uQcPIkRtFFNYALs1ErXhGsshG0twt+Jta3bypdLl9G11EV88zMM9kTbetDQMxORS1jPiI3M7t/PBfPBxH8ZVx4f/Frzyk+F+74Zzh1f3mg/5PfjZT4Qn/Bd47Ycd7fGtcfRx4sA9xEBKiYODDarnGWJkjJEpRbIEJoCq1FIIeUBScW8TM96SasDX0L15jYvjU1Hj3msQstvnlqZrd4BXoLShGxSXGhZynhinkUMH9904MU3ZwL0P5qhuJewA3y4JRMzut6pl7bGSaiU53w7mx1Jy4+1nxUnT8AcJZvQVwIy8TOkzeaNUUEjBADWE9sbBod1ImxnYo3eMyl72LjLbBxtd5XOp2nntdp2zMieEQBoSabPxbSC6j8yqb7/58TH/FV7y2XD72+79uZfHA98BP/bX4bNfslI0pz1OHrgLDClSt1tEhE0a2MXILgR2Kq7ttgxZ0oaQCloqGmeOuPHkzdUxLBJTA3fIwbxnpBlmOT+jTdJXTe5IzaaOKZkpj4zTxOE4cjiOfVJTniq5zFv10X+t0Ks+106C2uzUqMSoDBVKNGllAFKM5Gy8fcvcG2ESMUsGlUARoVQrlha1wdtTseJv87sJ/n/L1Gcbh3a/WQGIG30tnRqDn4NazeGxly/aOXIav/UDRJd4ps2G4WDrwB77NKazElGjfXFS1yu1jP1FnwOPesP17+aDfh/+yivg1X8JcuLkft417jFOHLiLWBYoIgwpUTbFxuOJWMaei2fttkkpSLHmIvNoySZ6dKvd0LJL/wXNQSkCSdT8Z2SemmSNQeaTomWWPJY8UkpmmkbGcWQ3TRzuJgf3bOP0qpKL0zjelVqwgRjqY49EZKZkoql6hhigmhVwihNlsoWk8ehmJYAfa0CDTWoyEiZQVcgVy96xDxQ1kAgEiegC2IP4HNbgI/C8CNqKooQmkvEFLuASVBfLt2HgrqQRL0RHz9yHzZbN9gBJcS6wnqHM/ev4On6Cn+C1vPa4D+WK8eG/Bf/5L1v2fV/jn/6f8CXfCp/4s/DbH37f93dS48M4u/zTyQJ3Md1GDJEwGD2jg4FbyYU8ZqbdSB4TMQ2UlAlpoKZK8BZT8W7S4Fr4oM2gF5oePlIts5e5k7LNDw3u6NjUKL2AmicH94mdb+OUmXKhZHUPGeuMrVXJ6jNa6apD04urEFTngmkNBJQUAkP2zD2XTs9oqbaA1YpEdR27dFcd2gBvNZ2+NWsFa9aSiIYEIZqN8CKbbpl7qwe0qrMVbF2lI9qvFlSrN0Yt+PumlAlh3yQsRpfAi/9YzwbAb2TLM3kmb+WtoPBcnsu75F3HfVgAPP6X4MV//8YAO8Bmgvd7Ozzn2fA3TkkN+WriM/nMDuiRyLP7+OezFycK3BsgGMA4SR4DmzIwbTaMw2hDH9xK1uwJEiEWaixQik04qg1L5+zS7FgUSrMKaIqW2UceH99HtWak0iwEsg3Jnqa5gDpOmWkqTLlYUbO2odQ+dHsB7tU/XOv+jCpodM/KaoA9SCbHRJkKZbIM3oZyj9Q0oKUgwe0JmNuO5jPnoNs5dHfWDMHPUSTEZMAe54LqPrgDWGG3Vu2Z9+wrL/01ewDv/QeSzh4VswwBnummpwr8Df4G/4H/wP/F/3W8x1XhL/9nePTrbvy+P+4X4KnfB9//FLpR6GmNJ/EkvpPv5EE86LgP5abEiQL3ZViyZw6GMQbndY3bDSkRkpuKpQQpIzkYYBYcpGoH7+qKxKpqAzRKcepj0XRUiwG/UzLZAX7KDuS5gXt7rDDlylQsY6/V3rp4Fl0WfjUN3PE6gNUjq9UEonH/OVTKZI1QeTdRdjvK4SFl2FiTU4zUELvqp/HxZm9s81VFol31xGgOmyn4sAxvnkoDMdgs03khoHeb2pWMcerumYYGIUSzYe7OnUuljV8XLYH/VggReCyP5Tf5zeM9ELXi59d/zdHs/gHvNA4f4KVP5VTz77dz+y0D7HCCwX0v3FwrRCHG5jQYCSkQUnCvX3NYNN549lbXUox6UePCc61kB1GzCigd4KtPappdHS0z3znIT7kwFQf1qkzVuPYO4n7r3oz2tS4H0NG7O0UhixKBQqVIJcdCGTNlNxqwHx5ShoEShJKSDdbegL2qElAiNlbQGJNoA0BiJHbNuS+Gw9CHZrTmpXamGmU1q91d3y+WrWk0PxkJ0m2ZuyyzyydP8V/9KY7Pfgl81xdCKkf3HqnAd3+Bff3Sv3t077PGjY1TAe59AIRL/EIKxGSZKXG+VQf3KpgW3O0CcN/zWtXB2cE9m+qmlGKF0OJTlErtwD6WzC4XxtycHiu5VrfyNcfJ2XTM1CtV1IzJGkjSLFeEIlYHKBVicSoHJYtSpmp0zG4kX9qRt5fIMTCJDb6QGNEoqAxotUUribAJ0UiZaFOPUrTzE1IkDLFTWbM80RuTmg9809O7vLHJHLuPPXjmLouCrNsb9AlVYdH5dIvEskvuJkco8NSXwrd+CZy/dPTvd+FO+LZnQQ3wHz8N3n3/o3/PGx7H+PM6jjgV4N41684XpxjJKRFTdHommpwk+vQkcQ8WNR5e3RKgVBuiMebGlVcz/ypt5qkrXUplKpnJgX3nz59cy56LZezVefZmzGWukxiwezenuxs4yPsDPmq0NCoHz96DF40PJ6Z0yLRJjCLEIMQhQUre2q8uRzRwryE63WJZekqRlIzGSh3cB+KwIbi/go3G885Td1cL2D4bVRPAFpNm+RCCz0Zt3Pqsuul/MK3d9xaIO7mTX+fXj+W9H+4dpeEmLqgPeCd839+F3//Ao+H3jzreyBt5M2/moTz0uA/lpsTJL5G49rwNtJi5dwP22IDGeWYciJpDZNZKrpaBj1NmzK5NL9mBvRVP7es+ZKMUplr67Vj9ftVOxVjWLbNXvNg8UrMjboOpr7CpdcoWmxViVwxNSrlQBY2XDpkOD5kuHTIeHjKNh+RppOTJMnetRBGGEBhiYgiRIUSS0zKp1yqCgf1gNYvoFgGt+CliQ6utgBr6ZKgQmhIm2Dkf7EogpNi99qUVUJttwS3kAvkn8id8m3zbzX9jhS/71zcX2FsEhQe/HZ7y0pv/3vc1flp+ml+QXzjuw7hpcQoyd6cMHNjTkEhlw1AqQx7IOROzgbwmm9ZUHeCN/zb/lakUd3Esxp97pt6y9arF7zsvX3MH+ZwLOVemUvtzap2LpXVxpMvt7j5Ns0pXbEhSDqZVz0WZstoitJvYDSMbLx6z3cJ2Y58/WvHYk2y7opHQC80xWiafYmCIYc7ikwOxWwdQnL834Y53+Vb3ofFrDTG6yfxjIjEmYrQO1LTZdLpoCfBrHG183T+BZ3778b3//d8Nn/Yf4Yf/JuwOju841rjnOAXg7uDVtNSbgY0PrWia8DoV6jCheaLEaL7pwTo5J+/gHLOB+5SzgXUrhtZGyVjLf/u6LDL7XAzQi0seq+K+8bOHlrsXtKWI1hZ1JcBvX89FVwzcK4xF2U2Vw5jZHE7EuIMY0e0OtjvqYB7wMiTToGMzUiVEA92YGDxrb+A+hMgQYy9GmyJ0YTdQBZFq3jc12KBwOpPuLa8BiZEUB4ZkNgPDZtO17XYVILcMJXNc8Yg3wsf+Vxjy8R7H338x/OkD4RV/Bf7jpx/vsaxx5Tj54O7yuhjNg2WjGwC0GoVRpkIZMiUNNr4uJjQkqkQygQlhrMquVuPNp9wtAqoabdPcFYvz8iUXc4Aspfu81FJ7s9CCpp4Ljl1zsgRy2Utm2wLQOO3O02OWZ5MKU4FdqRxOlWHMxDhBmtDDHWwPqZuIJmt8kjhYB2owiiWGyBAM3IcYja4JgRSFIUp3a1RcBi9GC4XapllVl/NUNwXTLoMn2JVBGjYMgzl2NnBPm8FqH6E1OK1xFHHbu+EFT4NP/LnjPhJbw//hv4ZP+WkrsJ6GH/sT9Al8FB91Ko71RsTJB3eaSiaSkplotSEUPWufMmUYqcOGGkdyTNSQKCEyIewUDktlLMahl1yaQMT80et+V6rdTvPXtdqUpNqKo8uJSPsZecvaRVxQuCgwir/GnjsDeyYQMRfHscJhhkghhAwSqGlEL+1gO6Abo57M8kOsuBmiF1MHUopsQmTrwD4EtwdwDp0YXJLpVEsVd3/UeWi4yzd7o5RgtgbDwLDZMmy3bA4O2Gw3pEYFJVfP3EIxi1xvThwcwhN/7qa+5b3Gh/4u/MuvhH/2L+Dw3HEfzT3Hh/AhPJJHHfdh3LQ4FeAexEBJgnV2hhCNWpiy6cI3E3mzoUwTedggaYBhoO4SOUQmEXZg2btz7HNDZpt2VPtM1OJGYerArsXtb6vOAO9Okq2Rdgb2OeSyh5ZulXix1QqwBvBBhVghFiVQkZ2Bu6YR2e6QSwnZmFpFgxBDIkR1X5jk+nYrrG46uAdT27Tz5/WINh1JJFCDzp7tfj5muwb/ZFFIjRbbbtgcbNkcbExLH2If0nGrZEUAF7nIE3gC/4X/clPe7zNO4KyQzQRf/o3w058C/+mvHffR3HvcQr+epwPccctbVTXuV8RolM2GvJkow8Q0bMjDRN6MxGGDpAlJg80mjYkigSxChj5r1Hsrqe6B3kG+tox94c3eRt3pgoJZAjxdLr533Ht3F8+11y8yeB/gnSuM+Gg/hBAykiJpnIi7kbRL5t8yJIgFGRS8ezQEo2ZSCCRxSiaY6drsAOxqHvUeVLdEqO34lPmKQ82LBzDljOvo42YgbQfSdmMeNYtu11vpj+eBPJAX8AJexat4M2/my/ly4Ggy+s95IXzLl57M8+vN1ydSBisIX6BfwCfxSTxKbp2sHU4BuF/JdCrGyJASebCJTWWayHnbu03zlNmUwjRNpHEkjiNhGJAxQaygZUbiJkavy87WBvI+6q6pY3q/j/d1Xusv8l2eP0+DMmWPNUXFqozavNKFOGWG3ciwS7ZtdsRhMLvj1oAUguvOmzwxzsT63jm0v8B2VSGIU13Se1TVv9moKqiztW/bvM+gfSh7yQn7y74J8YH+L5P5e/w9MpkP5UN5L++9oe/zF38d7veeG7rLGxrf/xT4fz8e/vqPH/eRWNzO7byMl/FIHslt3MY5TjhndARx4sF9GUugDy6L3GwG6sGWUjy7rtUoFbXxc7mYL8wwjUw5G4iG3Ac9U828q2vSdWEl0AqnFR923RJa2cvAr/FTAPtSyWWTU8UA3nxeCqEIacpspsw0TpTDkbLZUDcTusk2wQmH6yAQbIZqbZsDvDbTGF8M6vymXd9OkzTaNBC7OspCrdneQ+hzUvv81VsQ0K8UicT78X4UCp/JZ/JCXnjch3RT433eZZbDj/+lkzHk49k8m8fxOLtzi/6Knipw79EamnxAxKZ6x2az8FXzcqdaB2qTQJZSASGHEfFUXEtBczUViTcgGbhr3xolQ6Nm7CAuOyhttdTLHt5X0ey/bD+jtmEj5gMvalidamUs1We0TuTdQN1N1G2GbFbAloyL8fMhUHyT4EoaBHGrYapZ+fY5rOqNS1G8Mcx06wiUnBFRSjb7BsPxW6dJ6XoiEnkmz7zlwB1sQMin/Uf45cdz7IBa3fzjVlZv3au8QUS+V0TeJiKvWTz2z0XkzSLyG759+uJ7Xy0irxOR3xWRIyux2OCLwLAZ2J474NzF85y/eJELt13g4m0Xue22i9x2P7u9eNtFLly8wPmLFzh3/jwH584xHGytCWeTCIMpUMz6Niw6SfdBvtYG8jCnvW276zF25ueyx3SJ8v3lRs6ov1fWyqTKWKt1xk7WuVp2E+VwQncZnTKU2se0Ivj4wMgUImMI7CQwivi+3OystJGAbSCI+d+nGBm2G7YHW7bnrGA6bJLp482bgAV5dUQ/2TVOc3zV/w0f9arjPgr4Gr6G3+V3j/swjjWuJnN/AfBtwIsue/ybVfUblw+IyIcBTwE+HHh/4KdF5ENU9Qg868QaZ8TmrsYUregnEFSJnsU3nXouxk8HEXYpUqaRmjN5MpAttZp9cMlOS/iQCh+VN+sdG5DL4kgWRbTL1DNLIL9r5t5i1lM6xKNiXvSTYmZlxTX9Y6aOGR2zdT0VsyFAzcS+is2EncQGdlix1uTrodpCgIDU+XBRU9zEFBg2pmWnz+LO1Dw1cQ19UWuqmpWWWWMRBzv4vH8DH/j78ANPOb7jOOSQ2nvHb824V3BX1Z8XkUde5f6eDHy/qu6APxCR1wGPA37x+g/xbkKsc9X8xiGkgoiP1XPHRLzRKXtxtRQze48hkFMkTxMhmulV0ULMA6EUZMoQoyGgFySt0NpOyuWEynz/Svms3s3jV3rUGXGTeqp112b1bHvKlKlp+4tl7rUiTkM1+qgqZOhfV1VireYZo9X07bhOntZHIG40lhg2yQYzaKVkt1aurTh7z8e/BnwoH8or9ZV8JV/Ja/yCd2S8Za52vug74YNfB//ub0O5wcSvIGzYMDHd8uB9b3Ffuk6eJSK/6bTN+/pjDwX+aPGcN/ljdwkReYaI/KqI/Orb3/72a35zcbWKLPj3YWP66+25cxycO8fBhfOcv3iB8xdsu3jhIhcuXODCxYucv3DRHj9/noPz59meO8fm4MCpmo1xz51/vlHZqe79Y3Fr+bo5WRatFApFbcvFu2db9j7lPoaP5eYTpprKpbjap9Q2UGQBL83lcWHAtvSBj2H2o0mXf/+MTlq6UXE/7scn8Un8Ar/AO/zfR/KRx31YNzU+6WeOxv/mUTyKd/AOPoPPuPE7P2NxvX+l3wF8EPAXgLcA/+pad6Cqz1PVx6rqYx/84Adf+xE0YO8bhBRJLo/cnDswkD9/jnMO7hcc4C9euMjFixe5ePF+XLjtNi5cvI1zFy6yPX+O4dwBcbshDIMBfLe09VO1yN6XTMzdfs7Fk5dfz8DuoH7ZVrUayGs1gK/ND8fAvhYrBlMzUjLBbykZyoSW7Jvx6tosBmj1CrMCbh4xJq2M/UoGcPA3H/g0uI9MSqZtD70NbI0rhcBWtpz3f0/n6bdUcS9WeNKPw/u99cbuVxDOc55IvLE7PoNxXRdNqtp/ZCLy3UBTt74ZePjiqQ/zx25CuOojJeJmw4BBaKmVkit1LEhRkkTGwdwkc54YpwOGzZ0QoxUxqzKWwjgVQq5ITGaoJaa0afx4/zPVuycnGpgvaZn9ezPU164XtwlS7blWzBVKrQ7uBvDFrRK0ZCgjkkdCSUieO6qaJh8i7t1gNExwd81kvjDDYA6PfTHDRD7BpzqlYejSx2axHCTcirL2644v4os45JCv5CuP+1BuWvyVnzZ74LfdfuP2+R18BwAHrHaU9xbXBe4i8hBVfYvf/ZtAU9K8DPg+EfkmrKD6aOBX7vNRXv2BQYzEzdB93Q+Kotm2oLBNG6ZptAJlzeymkc2lAyQNFCojlZ0W96GpxF1BYkGkLNqN5IrdeNKk410Rs6eT2aNjZuiXuzzS/knn0AtFgw/ert2jvtYMeULKhOQRmazAbI5mrePKNmkWDpgVgV3hHDC4jXCM3pTUB2DbB4wxoYMSgn3PBqYkH4K9ovvVRiLx8Xw8j+AR/CF/eM2vf+3/Au+9ABfvOIKDO0XxYXwYAC/khfwkP8mf8WfHe0AnOO4V3EXkpcATgQeJyJuArwWeKCJ/AUOONwBfCKCqvyUiPwj8NlbTe+bRKGXu4XgBjZEgQkKouVKnik6FUJRNSOSypahlwZvJ7ApqEA7rxGHJHObMbppIYyFuzJkxh2jZuzcwLTP3K3y599iVM3eH8qY5Zx/c+zO1ohpsuLdWA3i3KW5GZ5oN3MkGwFKbaZkDdcCAXZPRVyG4CZjVKBrVMs9X9TMpRs2kwWwcTE4vTsusvPu1xsfwMXwv38tTeAp/wp9c02uf94U2oON/+Z2jObajii/+f5x7v8F5wL3RMk/mydzODbxkOIVxNWqZp17h4effw/O/Hvj6+3JQVx2L4dN70QZLAMNmg24LHGRCUaaUTB6p1ix0mCfCMJADXCojuzJxmEd248S0K0ybbJ41qVCnVp+fJYD3rn+4nKS/HO5xj5kFP6+XLwfGxLfMvWC+87k0cDeAZxqAgNb2l+SDw0tEXAZpfjWzN37aGI+OXJkRDiGiElj+Ld2qVgPXGu/iXUxMe489hsfw8/w8P8wP8zV8zTXt72kvgF/66NN1vfRZPwjP+jauu6P7Ahc6BfPVfDXvx/sBEAi8iBfxeXzeXV7zeB7P9/A9PJAHXvdxn4U4nR2qMI9z2+OWPTpQCRIDaZPgYEtQSCmZmgTrYoslQ4pkUS7l0TP3iXHMlLGQ78zk7UQZCzVXJLfW/dkW96gnyyn4YuTDtBvv3puRJsqUCSlb3YEIsXr3qi8We6en0SvmEbPMwpe6dXGztitp2VeAv/t4B+/gZ/gZvpav5bW89i7ffy7P5S285QqvvOf4nT9v3Z8f/cs34ihPRzyH5/DFfDHQpLv9L5sn8STextuu+LpwCiaIHnWcSnBXBywz9ardjxygmaDI4rniNIQohJTM5RGhakVqpsbApIUL045dnmzG6i5TdoXp/ES+cyTvJspUyFalXfDncJf8fQ/t7x75FwTI3mbdQ+0zGLdtx9uyd+s0nYp1rk6j3UoqBFE0+MwNWNLus7ZdZL6VNlDkykC9NildWxQKX8KX8H18390+50v4kuva97vvD3/v38ILPxc+7r9e7xGenngMj+GJPPFugXoJ9mvcNU4luANuEFaoPmpPq1H7CjSfchFB1KYJibfRx6oO7pYNSy1oimQqt5VMrpVSqjUKjZV8KRu4j5O9j+frpRSkMlsBwx7cL2NpOSN7kK77wC7s/bLOv7xmiWCukUquVuwds6l6dmMhjhWSkoKSEr44uIRTGpcu/b2Yz9YVi8NrXF8oyo/xY0e2/9d/MPzix1gH6EP++Mje5kTE7dzOI3nkcR/GqY3TC+6qZh8wjpRpcpsBB1cvBEoIpBB9ilMibBKodWmq7yPUCkOkiNEdVZValDopdaeMd46Mdx4y7UamKdu0Is3eQVpn68hlyHxfLhPWtOar9nVr+mxb8O+ADSkxyWFw10qjZSbP3MdS2U2FYaoG7kNFE1CFhA3Npmv0FyPwFlJObUM7buhPZ42jjK/4/8G3PQve8KjjPpKjjf/Ef+KFvJBn8szjPpRTGacT3JuRl2fullVny+bV7HvF1R+aNgyDoEmQwVQhc7atBK2QXGaYqw3BnpS8y0yHI5feeweH5+9gd+kS426klmL+7sU6Pq8m5Z2tCWbGsP1/CepBDILbq6RdgdAsEOiF4KzVsveijFM1gM9KqD7ko+3VLX+75UCje/QKi9IapyME/uRB8PzPh8//3vWia40rx+kEd7CCplMzpczt+LVWm5AkQvBhEiFFhgCSIhJD16P3hqQYe8auValZyWNm2o1ceu97uXTHHRweHjKOI6UW1Lv9Q4UqappyD/E97/3B7WXvLW2XPXAXcXZduuOLZe6ETteo0C2JsxpFM5V5S1UJxs6Y6Zn4SL3LyJjGwRu+6woOJyQ2bHj4Xg+gFf3/gD+4y3PvuAj/4Lnwfm87OQMy1jhZcXrBHeglwl7X9ClKtTqWKTXGRUavjdi2uawtgjDULdui1FIpU2XajewOL/He91zg3MXznLt0J+Pu0IZyZ6UURYu9VyBQtbJYMu4CmHu5uhcw52x9vl3y7DMtw7wCiA24tuFRzZb4suEiOP+/dxB3p8Jf46TE+/F+vJgX8zF8TH/s3bybB/JAMvkuz7/zArzsb8Cn/DScO7yZR7rGaYhTqxcSsKHM0S0H3PDKpgThmWntk5lqztRsY/i0D+DwLFukT3XabrccnD/H+Qvnbbt4gfMXznPuwjkOzh90n/O4cbdEHzwdFqaJtu0XS4ODtlEmNkAjyP4m4t+XQAitKBz8/syfawj7mflilJ76ojaXeBvUt4d0xfQTGm/iTTyNp/F3+DscYmh9jnM8W599tz+z5z8d3nPbTTzINU5NnM7MvXPqkZAGks4FVJ2AnC17V9DOy4+EIGgpboY167ub7jsOic3BllqVnCcOxzu5eMcF7rjjAoe7OxnHQ3Ip5FyZJlPplJoJKtQqpp5x24B+qNDH8hm10oBYOia7e7wNsp4Fiw7s81g7w/eASETECsUhzguMRF/YgqJSUSnsG5It/t21l2qNExD/w/+9l/fyE/wEAwP/lH/K23k738V33eX5KvDkH4Vf/NhjONg1TnSc2szdOHXrshy2W4aDg3m6UnMuxCWTOVPGkelwR97tKK0Au5e92wDotElsz204d+GA8xfPc/62C5y/3wW7ve0CBxfOsT2/ZTjYkDaJOERiskw7hCW9MtMsYbEtnSwDYoDum33N4jFXzARZgHpyUJ83s+K1UXkScbVOpY3dVmqnrFYwPx3xWl7LL/FLAGxkw4HcjVGWwB88Cn72iTfv2NY4HXFqwb1l7nEYSNuNAft2y7A1cI/RuWpVai6UcSIf7pgOR5NPjpNZ5i4ixEBKkeFgYHv+wOyCL57n/MXzXLjNbs9dPMfB+S3bg4HhIHWAT07RxGBgHJxTj0hXwQRZAL3Mz4sO8gILmiYYyAfvJm1XKiESYuqbzT0NtgUhBEWkiSaLe9OUBT0zz4Nd42gi3YAL4j/gD/gpfqoPpPhC/f/ycH34FZ/71j9n049+8aPXtXuNOU4lLdO6OCVGH4mHyxKh1kLKGdUyZ6u1UjT3iqM4eNaUCH0Yh9gkpxQZYqDWyrnzB5y/cJ4Lt11gd3iJ3eGOcbTu1XHnTU2lIlSygGSxMXe1EkqTW/o8VlqDVejF0bDg6IP4QuCyR0EMyMUmR6UYSW4XENPgt3OtIXjmbjudB3+YacFsQyDajmaNo4hI5BW8gmfxrL3Hf5Pf5E7uvKZ9fQPfwGfwGTyWx/Ln+fN8LB/LG3kjr+f1vJ39ATdvfCT81f8Eb3nIyXKO/O0PO+4juHXjVIL7XtONtIYfNXoiJUpKxFpAoeZC0/3VWiEXJBRCLIScbUDFYl8hmAJl2AxstlsOzh1w7tw5zl+4wO7SIeNuYjrMjIej7btaB2wMkTiN5CCEXKniw65bnxM23zQILk90ykZa/+lsC9Aal+wqwCiXFJNPRTIf9pgGKyA7sEur6DLbM+AL2zyKr+7LL2/Wz+sWCkH4S/wlfvGyyZJ/kb/Ib/Ab17SvkZHn8TxezstB4Fv5Vh7Mg/kRfoTP5XN5N+/ee/7hwfUbdB1VfPH/g41sXOOmx+kE97sJaRx0SgZqDpg1165hB6UUA3YZXenidIfEACH0AmsaTD1z/tx5xvM7pvM78mGm7DJlzEiFKMZ5T2nHuItMMVGmTHZlTq1qdgYAKhTrJKLZD6jO8sjGz0dXx8TgfHqKDCmRUmJoQzb8NrSrF29y0oUc1KYw1Z6xB1Ui2qWXK8DfvLje4RLfzXf3r3+en+cVvIIn82Tuz/3vAu4lwpd+i3Wvbnc2Dek449ueafWANY4nzha4+/i4NjlIYqRIJJNRzdZVqmrZuzQrVu0LQtq0iUMuj0yJ7WbL+YPzlPMT5baMZoUiUIQUEkPaMKQNu+GQ3XCJ8XDHOI7EcSJPRt0AUNUVM+oiyRng6W1LLoN0KmaIbYbp4IOrB7ui2GwYhq2PxmtzXi3/V1UfwVfRXCBVpBSCViIG7knUwD2Ij8tb46jjR/lRHsJD7tNQ51fzal7Da/gIPoIrGUZogBc8DV7y2fCznwgfe+PH0l9T/MGjTIu/xvHEmQF3aV2pMUIaXBppLLYi1IINj1ZFSwWyc/KVGCNsan8N0f5wYghs0sC57TnKOcvYmRT33iVKJIWBFBOX0kBKiRis0DnJoUsdPUMXc69c/mnrEuA1dFominniGLhbxr5xYB82G7abDZvB3q9NRdIQ3J4AQrXPWEuFUpFqHjpJlShmzd7cZrgiTKxxo2NguM/7+DP+jK/n63kpL70bizoD+HEL3/O/w+N+BdJNHZUzx/94NPyXJxzPe69hcWbAHZj178OA1EgIRs3UKuSkiIOeVqX0gmtFU7Su1SFRq3HcVtAMDGnDweYAPSjo+WpZe7UthEiKAzEkUtwQYyK4/tyMDxrtUamTZ+51kbd3gnQhj3Q6ZgiRISY2KZGGgc2wMXDfbtluN2w2G+PeY7QFCS/eKt37hqIO7nWmZVQJqC95rcy7wvtpiYmJkfFen/f9T4HnPBvu92442N2EA7ss/uBR8KrH3fz3XWOOswXu4LM+FY0RDWY7UIoSc6Fkq26qc9A1Fy8yKjUlozPagAtX1AwpoZstHFTIanS5j06SEIyrl/nWFgUhuNZcMO8ZxYBW3StB1cA9IkRt0kfL2FOMDCmyGRKbYbBt45JPz9yHwQZbW1dumOmVVkut6l73nrk7qIu2IR61F15XeD/6GBj46/z1Tsu8iTfx6/z6Ne/nh/lhXsJL7vV669I5eP//aQD/5f/qug75uiNHGyqyxvHGmQL3PljC1TSKuB68IjEbZdPATOven8eyq7Q/FiIxJoZhg27dd6ba7NVcMrmaGqaUanbDbndgMp0CNdt+tThFU3pxUxUryDq4pxBIYhz74AXUzZDYDgPDxkB+GJya8a9T8oJrtyq4jGbpXaimmtFaZ+sF79yVnDultTx365SlGxsXuciP8CP9/u/wO3wOn8OreNU17cf6FJR/wD/gy+XL7/6JAiXBD/9NeOpL4aH/8zoP/Dri8AC+7p/cmH39ED/E7zAPjn0QD+Jr+dobs/MzHmcK3O8SYo1JMVnBVDeVGgQtAdSpiohlvym6n8ts8WUcvtkSDFopZcumFLalMI4TU8mUksl5MnWKFqqR+0gtiBaiK1RSCMRYiLXYouAEeQSSitEw0jL2ge1mYLu120bFDN6kNWwG4pAc3AMxhLmrdeFJ04Z0dAJGMS4+m4umThM6RoK24qpLKkNYM/kjjj/v/64V3Ft8Jp/Jl3MP4O7xXz8OPu0/wk/9Nbj/u+D8pet6u6uO916Az3mRj/O9AfGf/V+LD+KDVnC/yjjb4A5GmfgcVRGo0aY2iZurSFsAhgGJyUCxv9iy9zAokYHBZ5cOJbMZN2zLlpwzueQO7qrVGpK0ErUyAEmEIUaGaWIshakqxQXwQgN3y9y3KXGwSRxsBg4OtkbBbDdsz5nlwbDZ+FDrZjngnamx0UFzR6s0szGEqmKeOaUiuaDjRIg709nXalc4KRGSNXMpiyuhNY4kHq+P59/x77gk14i4Ahf0Ah+lH8Wr5N4Xh//+vxpF8/Tnw7/+sqNrcnr7g+CLvsOuFo4qO3gv7+VX+VUey2OP5g3OUJxxcBckmGeMDkoQoaboGngHd4w7j96tKmGmKERMfSDRGp1ircQykXIibQeGKTFsBzZ5QymZqoWqtnCEaln6gDKgjCGwi4lUJiand7QaRRMRWwBCZJOSZe2bDQfbwTL2zcYy+M1A2gykTXJtvrgjpRmGNZOx4LWAIC6RFKEqlKqEUpEpU3ej2x8osSphqCSxc6FxzdxvRnwxX8w38A1c4urB/XE8jifwBB7Mg/kCvuDqMn//YT7/f7cZBN/1hUeDvf/jQ+Df/+0j2PEi3spb+V6+dwX3q4gzDe7i/5MYiFgnZzdAB9q8uTbxqHd6giO7j6CLVqgNmohlIE4Dw3YgTQbsuRqwGx9aTJVSs4M7toVESiMpR6ZSKNWkiqLWVBRFSCF2nn27HdgebBzcBzbbJoVMxCF1YLfFqX0Gepdto1eQQFWhKORqmnfNmThOtiCoTXdKqkgMhJj6517jZEUg8Dgex4fwIde9j+/9fKNo/un/aXTNjeLi3/z+8NkvuTH7WuPGxJkG96Y5tMw2zo9fXjuVRSYjskdHSDDOWlQJQyRkc45Mm8Rmu6GUQtHsBl0G7i1zD6WSqskPYwiEMRKnyFgKpVS02si+gBKDNS5tYmKz2bDZDmw2yYqpG+tM3Sx49hDmg27DPBq4N6BvH0N9LGFx3XvNhRqmnu1rsIavUApVa3e1XOOIQ+CpPJU/48+u6um3cRvfxDf1136ofihP5+kAvIbX8Mv88r3uoyT4ww+AL/huuP2t8B/+1n1vdvrvH2FF2zc+8r7tZ40bG2cb3Jl54+vijxfFVcXomei682GzIWfLwEu18X61VErJaC5WrNxO4BYACmgIEAXJxcf1VePo1Y3DgjCkxDBE0hD33SajWwo3Xn3PO2DpH28+MiZ1rNRaKVoppZBjtOPJTr1ka/qSXPtCtHqK3bwQhG/kG6/79Z8gn8An8AkAfDPffFXgvnhz3vrn4GkvgP/t1+BRfwDf8I/v8pR7DAX+8BFWQP2tj7iWI1/jZsSZB/cbGSEIMUWzAthuUB+tV7VStZJLYciJMiXiZiBuN9RSiKWaKiZGNAYohdhAv1aiuvGZiGncHdxDMkpIgtomSujmX0Ifr+QMEq5vdxv3blZWAxQRcgmoT6+qxW2ESyVUJS7G9K0Af/riGTyDH+fH+Rl+5ppe93sfYluajLJp8QXfDX/v397za//Zv4BXfjK844HXccBrHHms4H6VISJO8QRiGhi21WSGbvFbXB5pipNEGBKySZAH6xJ17j+kQCrFrAHcsbF1jUbP3qM/TwKoVJBiPaUq1GoKIN3jkgDEQJ2KFnfB1EoR93cX88OpQQglUkskREjWm0VUU9SsuH464wIX2LK97tfnAd52+3z/6/+JbWuc3ljB/RpDQiDGyGazsbmmQM6ZcZpIQzIL4RQhRRgSuhkMxBu45wSlEqz7CdFKrErEsvsg6gVRLCOnUhFqzdTammOtSBoCM8gH6U6YaEVDAbVu3e5xE4QaI9G5/qgAgaih9dK6fmiNNdY47XGv4C4iDwdeBNyOXbA/T1W/RUQeAPwA8EjgDcBnqeo7xcjtbwE+HbgTeJqq/trRHP7NjyABUjLnxpQQYJwmht1IGmyIhmXvA2yyAa6qKVtyIpZixl6uXAlVSbUStJrnixYDZCoalCpKUUyBU2y+kmhANBiF0tQ9VebiAAbkqskNpszXRkNAU0Rre9xMhqu0gdttkMjxnNs17lvcnZnYGrdmXE3mnoF/pKq/JiK3Aa8WkVcATwNeqarPEZFnA88Gvgr4NODRvj0e+A6/PRMhIqabD4HorfybcWSzOSQNA2nYEDeZmE0KGVUtK47R/NWLT++oFfEMPtVKdIAXNVklPkWpmqclRZWs4sZjfSIrgrqvjNFGzfqGapYKDesJEWK20YK1IupDQ2KEGFFpU61WZD+t8Vl8Fo/hMf3+G3gDP8APHOMRrXGcca/grqpvAd7iX79HRF4LPBR4MvBEf9oLgZ/DwP3JwIvUJk//koi8j4g8xPdzqmNPIulfp4V6Zthu2RyMrqBRsmJZcYiQMiE3rt3APTi4x1KJWpBaoASo2YqbpfZsLCiU6n44qqawIdCJlADmFl+7eVhAUCkQAloylIyUbP433rAUXNseYtz3mFnj1MXn8Xl791/BK1Zwv4Xjmjh3EXkk8BeBXwZuXwD2H2O0DRjw/9HiZW/yx/bAXUSeATwD4BGPeMS1HveJiRCDq2e2nDt3jlqrcfEhQJyHWbfBHepac6oNDQmetUspSCnUMlKz2/ZihmShmX1ptR4sEbMUdi49aCumzgIaI10qgYpKIYVMbZk7lRDEj9sao9JgE59CWHmZNdY4C3HV4C4iF4F/D3yZqr57mcWqqorINRF+qvo84HkAj33sY08tWSgIyRuPyvlzAMSYzKcmJEKwrHgaBzPrcmpGc4VUjJop1TLqUtBs3GnNlVKt0Fkw90n18moUH8eHyyNbKy6gatOY2iMRs58UKYSYoVgHbQrCsInmDX+wsc5Xb45ak/c11jj9cVXgLiIDBuwvUdX/4A+/tdEtIvIQ4G3++JuBhy9e/jB/7GyGT3/abDaADcpOw2CKGQlOfUSmNFF69l6ouaDZwF1qhZyoebK6qJoJWanZPGgU19JDRX2SUrVZq1W6pwxtzF61ND6ouJOAEqQ4324j91IMbIfEwcGGzXbjDVPuDb+i+5mIj+Kj+Gw+m5dwdnwBHspD+Qq+4rgP41TE1ahlBHg+8FpV/abFt14GfC7wHL/90cXjzxKR78cKqe86C3z7PYWN6dsQQnTP9Y15tPgQj+jgnseJnK2DtZSCTsUoGvdWr1OkiiBYh2nt4K6WtatZBUeUKEqppk8PBILOM1TVaZno2vdQlCguvSxmLzDEwMadJ4eDjbtKuq0BqyPkWYj34X14CA857sO4caHwCXwCj5JHHfeRnIq4msz944C/D/x3EfkNf+wfY6D+gyLydOCNwGf5916OySBfh0kh96s8ZzCCZ7sxRrQOxGRm1lXVpStCDCM5JkqeqLlQcqamAnkeZp3jhAZhwpUzNfvkKHOnrOpTojCte+wuMC5jRGxG7IIzF7z7tKp1r6rp3lMMDENkcP+aed4rrJz72Ykn8STux/34Z/yz4z6U64q/qX+Tj+Xj+m/kM/jCYz2e0xRXo5b5f7n7v/ZPvsLzFXjmfTyuUxN3UdAEIAilbtnmagZhanRNHsyawMDdqBmK+b/UqZCmCRkiJShFlOzj+UqfsFRAfDyed8eqW/p297BgYndts/NE50Hc2rxyZs/3mEIvoq7J+tmLv8xf5uP4OJ7MkwF4u76dJ/Nk7pAjMnW/wfHJfDJf7HCy/n5eW6wdqjcoliAfQ2BIA9utGYqJCClZ1l5yoTrnXkuBYhx5A/zh8BCSGXupmCtlDjtvMMqQbXTf7NsufTBHz9gv/yOQ2jtb59HYPnnJh5OsfzhnNxKJj+QjASvKP5dv5el8/r286oTE5S4ba1x1rOB+RBFCIKXEdrtBRBhSpuSB6oZh1f1ltLaZptXBfYAkEC0DF4EsQkG8k9RsBoLYeMA2iUmidP8ZFWtQCt7khFYIoCFSmaczrX82t16I/16scfZjBfcjihCEYYjAQAiBWgYfxeeDqp0DbwOrqUophc2lDcFtfkXNKXIXIlkiJeyoIaIlG9PuPjcxWQaubihWnYMPmG2BqDqWJwgJlejPpXP4a6yxxtmKFdyPKFqBFYyS2QNyFl9715EqaK1sthvzoRFBPUMfQmCUQI6BsovUkm3At4hz5k7LiNEutQ3HxhaIDvApQRhA4oKeWWONNc5irOB+RCGLQdW6zI7VDZ50fqAP/atqWX41Pr6MGamVRCAiTCFQYqRME6rm4xh8hiq9uBqdvomIOLh7l6vEBNGy90bNrOB+68XH8/G8gBfwpXwp7+Jdx304axxRrOB+BHG5RvyaNOOqTNNEnjJlykAlhkgMgTwk8rgxcK8FrYXGy4PZEiiBGiI+5QOpSqj4sJBE3GwIaXAd/sq93orxwXwwH8QH8Vf5q9TmW3EV8cP8MF/Clxzhka1xI2MF9yOM62kESkNis92Szxm4C2ZOttkkpt0BZRpdK58puYG88fi1ucmEYDNjJSAKopAQhpgYtgekzdZsiUNcm5Vu0RDkmhuc3pf3PaKjWeMoYgX3ExYSAilFNtsN9cI5QhSGzcB0sHX7gkzNmTyO5HFHnkbKlKm1UKo1MNUmcZSAqPvQSGSIic3BAcPBljgMhLha/K5x9fEEnsAn8on8LD973IeyxlXECu4nMGJKVlhFGIZEOZfJUzYZZSnUWhh3O6ZLh+wuXWLa7cx1spr/jDU1GcALQpBICpEhDQ7uByS3SFgz9zWuNh7BI/gAPuC4D2ONq4wV3E9ghBDMfCwEho1LKGsxCwGXUo6Hhxy+907XukfkcCSUbPYDYnNW2xZDIsVEShs22y3DdkscEhLCyruvscYZjRXcT2CYtNGnN/XxSi6hBGqtpGEwQ4JaXD8PkoM5Qnp/kkgghOTgvrGhIlu7jd4AJSsts8Y1xD/Uf8hP8pP8ifzJ3T4nk2/iEa1xd7GC+wkLQVBRG6EX7wl4hTJlhsND8jBRsnnOSJgtCIIXVmNMxJRImw0x+ZzXPnnppnysNc5IfCQfye/xe/c4r/VZPIsX8+J1pusxxwruJy2Eq8qmU0rEIRF9RJ5l+5E2MkU6uPsovWDPCTHMdMxKyaxxrSFwkYv3+JRv59t5KS9lYrpJB7XGlWIF91MaZhzmgO3ceggCVVC0z0Nt3a7ijU5WaPXHjvtDrLHGGkcWoifAW0RE3g7cAdw9kbcGwINYz9HVxHqeri7W83R1cZLP0weo6oOv9I0TAe4AIvKrqvrY4z6OkxzrObq6WM/T1cV6nq4uTut5Csd9AGusscYaa9z4WMF9jTXWWOMMxkkC9+cd9wGcgljP0dXFep6uLtbzdHVxKs/TieHc11hjjTXWuHFxkjL3NdZYY401blAcO7iLyKeKyO+KyOtE5NnHfTzHGSLyvSLyNhF5zeKxB4jIK0Tk9/z2ff1xEZHn+nn7TRH5347vyG9eiMjDReRnReS3ReS3RORL/fH1PC1CRA5E5FdE5L/5efo//PFHicgv+/n4ARHZ+ONbv/86//4jj/UD3OQQkSgivy4iP+73T/15OlZwF5EIfDvwacCHAU8VkQ87zmM65ngB8KmXPfZs4JWq+mjglX4f7Jw92rdnAN9xk47xuCMD/0hVPwz4aOCZ/juznqf92AGfpKqPAf4C8Kki8tHA/w18s6p+MPBO4On+/KcD7/THv9mfdyvFlwKvXdw//eepzfI8jg34GOCnFve/Gvjq4zym496ARwKvWdz/XeAh/vVDgN/1r78LeOqVnncrbcCPAn9lPU/3eI7OA78GPB5rxkn+eP/7A34K+Bj/Ovnz5LiP/Sadn4dhCcEnAT+OOS6d+vN03LTMQ4E/Wtx/kz+2xhy3q+pb/Os/Bm73r2/5c+eXxH8R+GXW83SXcKrhN4C3Aa8AXg/8mao228bluejnyb//LuCBN/WAjy/+NfCV0GcOPpAzcJ6OG9zXuIZQSxdWeRMgIheBfw98maq+e/m99TxZqGpR1b+AZaaPA/788R7RyQsReRLwNlV99XEfy42O4wb3NwMPX9x/mD+2xhxvFZGHAPjt2/zxW/bciciAAftLVPU/+MPrebqbUNU/A34WoxfeR0SaYeDyXPTz5N+/P/CnN/dIjyU+DvgbIvIG4PsxauZbOAPn6bjB/VXAo70yvQGeArzsmI/ppMXLgM/1rz8X45jb45/japCPBt61oCXObIjNBXw+8FpV/abFt9bztAgRebCIvI9/fQ6rS7wWA/m/7U+7/Dy18/e3gZ/xK6AzHar61ar6MFV9JIY/P6Oqn81ZOE/HTfoDnw78D4wP/JrjPp5jPhcvBd4CTBjP93SMz3sl8HvATwMP8OcKpjR6PfDfgcce9/HfpHP08Rjl8pvAb/j26et5ust5+kjg1/08vQb4Z/74BwK/ArwO+CFg648f+P3X+fc/8Lg/wzGcsycCP35WztPaobrGGmuscQbjuGmZNdZYY401jiBWcF9jjTXWOIOxgvsaa6yxxhmMFdzXWGONNc5grOC+xhprrHEGYwX3NdZYY40zGCu4r7HGGmucwVjBfY011ljjDMb/H4gb2ag0QXCLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DIR = \"PreliminaryGenerativeHistoPath/cyto2label_public/train/\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for filename in os.listdir(DIR):\n",
    "    path = DIR + filename\n",
    "    img = cv2.imread(path)\n",
    "    imgs.append(img)\n",
    "    \n",
    "    \n",
    "print(len(imgs))\n",
    "print(imgs[0][175, 300, :]) # B is stored first by cv2\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "pixel = {\"cytoplasm\" : np.array([0, 255, 0]), \"nucleus\" : np.array([255, 0, 0])}\n",
    "DIR = \"data/train/\"\n",
    "instance = {}\n",
    "filename = \"100.png\"\n",
    "mask = cv2.imread(DIR + \"masks/\" + filename)\n",
    "\n",
    "instance[\"file_name\"] = DIR = \"imgs/\" + filename\n",
    "instance[\"height\"], instance[\"width\"], x = mask.shape\n",
    "instance[\"image_id\"] = filename.split(\".\")[0]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 226, 226]\n",
      "[17, 0, 54, 11]\n",
      "[35, 155, 94, 211]\n",
      "[137, 70, 174, 129]\n"
     ]
    }
   ],
   "source": [
    "from skimage.measure import label\n",
    "from pycocotools.mask import encode\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def getComponents(mask):\n",
    "    labels, num = label(mask, return_num=True)\n",
    "    object_masks = []\n",
    "    for i in range(1, num+1):\n",
    "        object_masks.append(np.array((labels == i), dtype=np.uint8))\n",
    "    return object_masks\n",
    "\n",
    "annos = []\n",
    "for ID, cls in enumerate(pixel):\n",
    "    bn_mask = np.array((mask == pixel[cls]).all(axis=2), dtype=int)\n",
    "    object_masks = getComponents(bn_mask)\n",
    "   \n",
    "    for object_mask in object_masks:\n",
    "        annotation = {}\n",
    "        rle = encode(np.asarray(object_mask, order=\"F\"))\n",
    "        points = np.argwhere(object_mask > 0)\n",
    "        x, y = points[:, 0], points[:, 1]\n",
    "        \n",
    "        annotation[\"bbox\"] = [np.min(x), np.min(y), np.max(x), np.max(y)]\n",
    "        print(annotation[\"bbox\"])\n",
    "        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "        annotation[\"category_id\"] = ID\n",
    "        annotation[\"segmentation\"] = rle\n",
    "        annos.append(annotation)\n",
    "        \n",
    "        \n",
    "instance[\"annotations\"] = annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'imgs/100.png',\n",
       " 'height': 227,\n",
       " 'width': 227,\n",
       " 'image_id': '100',\n",
       " 'annotations': [{'bbox': [0, 0, 226, 226],\n",
       "   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n",
       "   'category_id': 0,\n",
       "   'segmentation': {'size': [227, 227],\n",
       "    'counts': b'R4n0U600:kJROa3n0_LROa3n0_LROa3n0_LROa3n0_LRO\\\\3Z1SL]O`3c0`L]O`3c0`L]O`3c0^L_Ob3a0^L_Ob3Q200000000000000000000000002N00000000000000000`M[Mb0e2^O[Mb0e2^O[Mb0e2^O[Mb0e2^O[Mb0e2TOTN=l1CTN=l1CTN=l1CTN=l1CTN=l1^OaN7b1I^N7b1I^N5d1K\\\\N5d1K\\\\N5d1CkN6U1JkN6U1JkN6U1JkN6U1JkN6U1HPO5P1KPO5P1HUO6k0JUO6k0JUO6k0JUO6k0JUO6k0JXO3h0MXO3h0MXO3h0MZO1f0OZO1f0MDK<5DK<5DK<5DKeM0i15b0KeM0i10i0N[M7j1Kk0N[M7j1Kk0N[M7j1Kk0N[M:g1Hn0N[M:g1CS10YM=d1CS10YM=d1CS10YM=d1CS10YM?b1AU10YM?b1An1l0eMTO[2l0eMTO[2l0eMTO[2Q1`MoN`2Q1`MlNc2T1[MnNe2R1[MnNe2R1[MnNe2R1[MnNe2R1[MnNh2o0SMVOm2j0SMVOm2j0SMVOm2j0SMVOm2j0SMVOm2g0VMYOj2g0VMYOj2g0VMYOl2c0VM]Oj2c0VM[Ol2e0TM[Ol2e0TM[Ol2e0TM[Oo2?TMAl2?TMAl2=SMFm2:SMFm2:SMFm2:SMFm2:SMFo28QMHo28QMHo28QMHo28QMHo28QMHR32oL0Q30oL0Q30oL0Q30lL3T3MlL3T3MlL3T3MlL3T3MlL3V3KhL7X3IhL7X3GjL9V3GjL9V3GjL9P5000000000K500000000M300N20000A?00000002K300000000gLdNi0^1WObNi0^1POnNVONKW1o0kNVONKW1o0kNVONKW1l0nN\\\\OJFX1n0nN\\\\OJFX1l0PO^OKTOd1^1aN^OKTOd1^1aN^OKTOd1^1cN\\\\OIVOd1^1cN\\\\OIVOd1[1fN_OZ2a0fM_OZ2a0fM_OZ2?hM^O[2b0eM^O[2b0eM\\\\O]2d0cM\\\\O]2d0cM\\\\O]2a0fM_OZ2a0fM_O]29hMB]2>cMB]2>cMB]27jMIV27jMIV22oMNQ22oMNQ22oMNQ2ORN1n1ORN1S2HoM8Q2HoM8Q2HoM8Q2HRN5n1KRN5S2FoM5T2KlM5T2KlM5T2KoM2Q2NoM2Q20mM0S20mM0S20mM0S20mM0S20mM0U23fMH_28aMH_28aMH_2;^MEb2;^MEe2:TMDS3<mLDS3<mLDU3=cLH]38cLH]3o100003E800000007B703M00005C80hN'}},\n",
       "  {'bbox': [17, 0, 54, 11],\n",
       "   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n",
       "   'category_id': 0,\n",
       "   'segmentation': {'size': [227, 227],\n",
       "    'counts': b'a0V1m5000000000005H30000N20_d_1'}},\n",
       "  {'bbox': [35, 155, 94, 211],\n",
       "   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n",
       "   'category_id': 1,\n",
       "   'segmentation': {'size': [227, 227],\n",
       "    'counts': b'n]R13P70I<K000M300N2000002N0M30000N200000000M300K50000I700K50000M300N2000003M002N00003M02N000000005K00003M002I50003H50kX3'}},\n",
       "  {'bbox': [137, 70, 174, 129],\n",
       "   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n",
       "   'category_id': 1,\n",
       "   'segmentation': {'size': [227, 227],\n",
       "    'counts': b'nd?:i60M5N00003M000000002N00=C00005K000000000003M000000000M30002L200000003J300N200000002N000000003J300000000000002N000N2000[[e0'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from convert_coco import get_cell_dicts\n",
    "\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"cell_\" + d, lambda d=d: get_cell_dicts(\"data/\" + d + \"/\"))\n",
    "    MetadataCatalog.get(\"cell_\" + d).set(thing_classes=[\"cytoplasm\", \"nucleus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/02 23:48:26 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[07/02 23:48:27 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 100 images left.\n",
      "\u001b[32m[07/02 23:48:27 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "| cytoplasm  | 198          |  nucleus   | 221          |\n",
      "|            |              |            |              |\n",
      "|   total    | 419          |            |              |\u001b[0m\n",
      "\u001b[32m[07/02 23:48:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/02 23:48:27 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/02 23:48:27 d2.data.common]: \u001b[0mSerializing 100 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/02 23:48:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.22 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/02 23:48:27 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarushii/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/detectron2/data/detection_utils.py:419: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
      "/home/tarushii/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/detectron2/data/detection_utils.py:419: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/02 23:48:32 d2.utils.events]: \u001b[0m eta: 0:17:54  iter: 19  total_loss: 2.732  loss_cls: 0.9202  loss_box_reg: 0.6662  loss_mask: 0.6913  loss_rpn_cls: 0.3781  loss_rpn_loc: 0.06157  time: 0.2174  data_time: 0.0260  lr: 9.9905e-05  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:48:36 d2.utils.events]: \u001b[0m eta: 0:17:56  iter: 39  total_loss: 2.034  loss_cls: 0.4896  loss_box_reg: 0.6058  loss_mask: 0.6811  loss_rpn_cls: 0.1694  loss_rpn_loc: 0.06242  time: 0.2172  data_time: 0.0049  lr: 0.0001998  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:48:40 d2.utils.events]: \u001b[0m eta: 0:17:47  iter: 59  total_loss: 1.777  loss_cls: 0.3498  loss_box_reg: 0.5267  loss_mask: 0.6712  loss_rpn_cls: 0.1161  loss_rpn_loc: 0.04726  time: 0.2168  data_time: 0.0047  lr: 0.0002997  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:48:45 d2.utils.events]: \u001b[0m eta: 0:17:40  iter: 79  total_loss: 1.671  loss_cls: 0.2906  loss_box_reg: 0.436  loss_mask: 0.6537  loss_rpn_cls: 0.1725  loss_rpn_loc: 0.06554  time: 0.2155  data_time: 0.0049  lr: 0.00039961  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:48:49 d2.utils.events]: \u001b[0m eta: 0:17:33  iter: 99  total_loss: 1.71  loss_cls: 0.3453  loss_box_reg: 0.5042  loss_mask: 0.6097  loss_rpn_cls: 0.09625  loss_rpn_loc: 0.05024  time: 0.2153  data_time: 0.0047  lr: 0.00049951  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:48:53 d2.utils.events]: \u001b[0m eta: 0:17:22  iter: 119  total_loss: 1.538  loss_cls: 0.2907  loss_box_reg: 0.4722  loss_mask: 0.5674  loss_rpn_cls: 0.1456  loss_rpn_loc: 0.04127  time: 0.2146  data_time: 0.0046  lr: 0.00059941  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:48:58 d2.utils.events]: \u001b[0m eta: 0:17:22  iter: 139  total_loss: 1.298  loss_cls: 0.2495  loss_box_reg: 0.3488  loss_mask: 0.5072  loss_rpn_cls: 0.113  loss_rpn_loc: 0.05107  time: 0.2157  data_time: 0.0046  lr: 0.0006993  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:02 d2.utils.events]: \u001b[0m eta: 0:17:18  iter: 159  total_loss: 1.323  loss_cls: 0.2719  loss_box_reg: 0.3777  loss_mask: 0.5098  loss_rpn_cls: 0.06988  loss_rpn_loc: 0.03739  time: 0.2159  data_time: 0.0046  lr: 0.00079921  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:07 d2.utils.events]: \u001b[0m eta: 0:17:17  iter: 179  total_loss: 1.313  loss_cls: 0.3121  loss_box_reg: 0.4147  loss_mask: 0.4517  loss_rpn_cls: 0.1217  loss_rpn_loc: 0.06239  time: 0.2163  data_time: 0.0048  lr: 0.0008991  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:11 d2.utils.events]: \u001b[0m eta: 0:17:15  iter: 199  total_loss: 1.384  loss_cls: 0.3195  loss_box_reg: 0.3642  loss_mask: 0.4194  loss_rpn_cls: 0.07999  loss_rpn_loc: 0.04281  time: 0.2166  data_time: 0.0047  lr: 0.00099901  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:15 d2.utils.events]: \u001b[0m eta: 0:17:12  iter: 219  total_loss: 1.344  loss_cls: 0.2917  loss_box_reg: 0.3886  loss_mask: 0.4041  loss_rpn_cls: 0.09399  loss_rpn_loc: 0.06085  time: 0.2171  data_time: 0.0051  lr: 0.0010989  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:20 d2.utils.events]: \u001b[0m eta: 0:17:10  iter: 239  total_loss: 1.202  loss_cls: 0.2591  loss_box_reg: 0.3534  loss_mask: 0.3861  loss_rpn_cls: 0.07816  loss_rpn_loc: 0.04822  time: 0.2176  data_time: 0.0048  lr: 0.0011988  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:24 d2.utils.events]: \u001b[0m eta: 0:17:05  iter: 259  total_loss: 1.218  loss_cls: 0.2471  loss_box_reg: 0.3888  loss_mask: 0.3654  loss_rpn_cls: 0.08102  loss_rpn_loc: 0.04674  time: 0.2174  data_time: 0.0050  lr: 0.0012987  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:29 d2.utils.events]: \u001b[0m eta: 0:17:01  iter: 279  total_loss: 1.281  loss_cls: 0.3102  loss_box_reg: 0.3841  loss_mask: 0.3614  loss_rpn_cls: 0.06931  loss_rpn_loc: 0.04478  time: 0.2171  data_time: 0.0050  lr: 0.0013986  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:33 d2.utils.events]: \u001b[0m eta: 0:16:56  iter: 299  total_loss: 1.26  loss_cls: 0.2985  loss_box_reg: 0.3538  loss_mask: 0.3382  loss_rpn_cls: 0.1071  loss_rpn_loc: 0.0451  time: 0.2169  data_time: 0.0049  lr: 0.0014985  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:37 d2.utils.events]: \u001b[0m eta: 0:16:51  iter: 319  total_loss: 1.021  loss_cls: 0.2723  loss_box_reg: 0.316  loss_mask: 0.2767  loss_rpn_cls: 0.08173  loss_rpn_loc: 0.05509  time: 0.2171  data_time: 0.0048  lr: 0.0015984  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:42 d2.utils.events]: \u001b[0m eta: 0:16:50  iter: 339  total_loss: 1.149  loss_cls: 0.2508  loss_box_reg: 0.3685  loss_mask: 0.2836  loss_rpn_cls: 0.06297  loss_rpn_loc: 0.04261  time: 0.2178  data_time: 0.0049  lr: 0.0016983  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:47 d2.utils.events]: \u001b[0m eta: 0:16:51  iter: 359  total_loss: 1.198  loss_cls: 0.3018  loss_box_reg: 0.4446  loss_mask: 0.3521  loss_rpn_cls: 0.07464  loss_rpn_loc: 0.0475  time: 0.2186  data_time: 0.0049  lr: 0.0017982  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:51 d2.utils.events]: \u001b[0m eta: 0:16:48  iter: 379  total_loss: 1.105  loss_cls: 0.2959  loss_box_reg: 0.3835  loss_mask: 0.2772  loss_rpn_cls: 0.06308  loss_rpn_loc: 0.04839  time: 0.2188  data_time: 0.0049  lr: 0.0018981  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:49:56 d2.utils.events]: \u001b[0m eta: 0:16:44  iter: 399  total_loss: 1.286  loss_cls: 0.2933  loss_box_reg: 0.4185  loss_mask: 0.2479  loss_rpn_cls: 0.1446  loss_rpn_loc: 0.07424  time: 0.2192  data_time: 0.0049  lr: 0.001998  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:00 d2.utils.events]: \u001b[0m eta: 0:16:43  iter: 419  total_loss: 1.049  loss_cls: 0.2571  loss_box_reg: 0.3783  loss_mask: 0.249  loss_rpn_cls: 0.0697  loss_rpn_loc: 0.04092  time: 0.2195  data_time: 0.0049  lr: 0.0020979  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:05 d2.utils.events]: \u001b[0m eta: 0:16:38  iter: 439  total_loss: 1.21  loss_cls: 0.3358  loss_box_reg: 0.3886  loss_mask: 0.2676  loss_rpn_cls: 0.09049  loss_rpn_loc: 0.03595  time: 0.2194  data_time: 0.0049  lr: 0.0021978  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:09 d2.utils.events]: \u001b[0m eta: 0:16:35  iter: 459  total_loss: 1.497  loss_cls: 0.366  loss_box_reg: 0.5364  loss_mask: 0.3602  loss_rpn_cls: 0.1029  loss_rpn_loc: 0.03906  time: 0.2196  data_time: 0.0049  lr: 0.0022977  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:13 d2.utils.events]: \u001b[0m eta: 0:16:30  iter: 479  total_loss: 1.112  loss_cls: 0.2401  loss_box_reg: 0.3125  loss_mask: 0.297  loss_rpn_cls: 0.09382  loss_rpn_loc: 0.04267  time: 0.2195  data_time: 0.0048  lr: 0.0023976  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:18 d2.utils.events]: \u001b[0m eta: 0:16:25  iter: 499  total_loss: 1.034  loss_cls: 0.2608  loss_box_reg: 0.3378  loss_mask: 0.2833  loss_rpn_cls: 0.07635  loss_rpn_loc: 0.0507  time: 0.2196  data_time: 0.0048  lr: 0.0024975  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:22 d2.utils.events]: \u001b[0m eta: 0:16:21  iter: 519  total_loss: 1.019  loss_cls: 0.2444  loss_box_reg: 0.3611  loss_mask: 0.2656  loss_rpn_cls: 0.05588  loss_rpn_loc: 0.03772  time: 0.2196  data_time: 0.0049  lr: 0.0025974  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:27 d2.utils.events]: \u001b[0m eta: 0:16:17  iter: 539  total_loss: 1.176  loss_cls: 0.3119  loss_box_reg: 0.4079  loss_mask: 0.2308  loss_rpn_cls: 0.07684  loss_rpn_loc: 0.0464  time: 0.2198  data_time: 0.0048  lr: 0.0026973  max_mem: 2113M\n",
      "\u001b[32m[07/02 23:50:31 d2.utils.events]: \u001b[0m eta: 0:16:13  iter: 559  total_loss: 1.044  loss_cls: 0.254  loss_box_reg: 0.3882  loss_mask: 0.2005  loss_rpn_cls: 0.05579  loss_rpn_loc: 0.04143  time: 0.2201  data_time: 0.0049  lr: 0.0027972  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:50:36 d2.utils.events]: \u001b[0m eta: 0:16:10  iter: 579  total_loss: 1.16  loss_cls: 0.3192  loss_box_reg: 0.4551  loss_mask: 0.2036  loss_rpn_cls: 0.05737  loss_rpn_loc: 0.04856  time: 0.2204  data_time: 0.0049  lr: 0.0028971  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:50:41 d2.utils.events]: \u001b[0m eta: 0:16:08  iter: 599  total_loss: 1.116  loss_cls: 0.2897  loss_box_reg: 0.4207  loss_mask: 0.307  loss_rpn_cls: 0.09007  loss_rpn_loc: 0.0435  time: 0.2204  data_time: 0.0048  lr: 0.002997  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:50:45 d2.utils.events]: \u001b[0m eta: 0:16:04  iter: 619  total_loss: 1.17  loss_cls: 0.3205  loss_box_reg: 0.4492  loss_mask: 0.2582  loss_rpn_cls: 0.05984  loss_rpn_loc: 0.03701  time: 0.2205  data_time: 0.0049  lr: 0.0030969  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:50:50 d2.utils.events]: \u001b[0m eta: 0:16:02  iter: 639  total_loss: 1.175  loss_cls: 0.3198  loss_box_reg: 0.4312  loss_mask: 0.1869  loss_rpn_cls: 0.06329  loss_rpn_loc: 0.04733  time: 0.2207  data_time: 0.0049  lr: 0.0031968  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:50:54 d2.utils.events]: \u001b[0m eta: 0:15:57  iter: 659  total_loss: 0.9785  loss_cls: 0.2542  loss_box_reg: 0.3945  loss_mask: 0.2  loss_rpn_cls: 0.04567  loss_rpn_loc: 0.03483  time: 0.2209  data_time: 0.0049  lr: 0.0032967  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:50:59 d2.utils.events]: \u001b[0m eta: 0:15:54  iter: 679  total_loss: 1.201  loss_cls: 0.3328  loss_box_reg: 0.4469  loss_mask: 0.2234  loss_rpn_cls: 0.06236  loss_rpn_loc: 0.06361  time: 0.2211  data_time: 0.0049  lr: 0.0033966  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:03 d2.utils.events]: \u001b[0m eta: 0:15:51  iter: 699  total_loss: 1.29  loss_cls: 0.2843  loss_box_reg: 0.5393  loss_mask: 0.2155  loss_rpn_cls: 0.06509  loss_rpn_loc: 0.05255  time: 0.2212  data_time: 0.0047  lr: 0.0034965  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:08 d2.utils.events]: \u001b[0m eta: 0:15:48  iter: 719  total_loss: 1.261  loss_cls: 0.3079  loss_box_reg: 0.6122  loss_mask: 0.2413  loss_rpn_cls: 0.06809  loss_rpn_loc: 0.04766  time: 0.2214  data_time: 0.0048  lr: 0.0035964  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:12 d2.utils.events]: \u001b[0m eta: 0:15:43  iter: 739  total_loss: 1.127  loss_cls: 0.2596  loss_box_reg: 0.3782  loss_mask: 0.2021  loss_rpn_cls: 0.05482  loss_rpn_loc: 0.05357  time: 0.2213  data_time: 0.0049  lr: 0.0036963  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:17 d2.utils.events]: \u001b[0m eta: 0:15:39  iter: 759  total_loss: 1.015  loss_cls: 0.2644  loss_box_reg: 0.3767  loss_mask: 0.2085  loss_rpn_cls: 0.04064  loss_rpn_loc: 0.04689  time: 0.2214  data_time: 0.0048  lr: 0.0037962  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:21 d2.utils.events]: \u001b[0m eta: 0:15:34  iter: 779  total_loss: 1.086  loss_cls: 0.298  loss_box_reg: 0.4417  loss_mask: 0.1668  loss_rpn_cls: 0.06748  loss_rpn_loc: 0.05192  time: 0.2213  data_time: 0.0050  lr: 0.0038961  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:26 d2.utils.events]: \u001b[0m eta: 0:15:30  iter: 799  total_loss: 1.041  loss_cls: 0.2351  loss_box_reg: 0.4201  loss_mask: 0.1929  loss_rpn_cls: 0.04606  loss_rpn_loc: 0.03744  time: 0.2215  data_time: 0.0048  lr: 0.003996  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:30 d2.utils.events]: \u001b[0m eta: 0:15:26  iter: 819  total_loss: 1.14  loss_cls: 0.2842  loss_box_reg: 0.4988  loss_mask: 0.1961  loss_rpn_cls: 0.04348  loss_rpn_loc: 0.04519  time: 0.2215  data_time: 0.0049  lr: 0.0040959  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:35 d2.utils.events]: \u001b[0m eta: 0:15:23  iter: 839  total_loss: 1.048  loss_cls: 0.2307  loss_box_reg: 0.4178  loss_mask: 0.1921  loss_rpn_cls: 0.03879  loss_rpn_loc: 0.04502  time: 0.2217  data_time: 0.0049  lr: 0.0041958  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:40 d2.utils.events]: \u001b[0m eta: 0:15:19  iter: 859  total_loss: 1.066  loss_cls: 0.2448  loss_box_reg: 0.4495  loss_mask: 0.199  loss_rpn_cls: 0.05025  loss_rpn_loc: 0.04342  time: 0.2219  data_time: 0.0048  lr: 0.0042957  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:44 d2.utils.events]: \u001b[0m eta: 0:15:14  iter: 879  total_loss: 0.957  loss_cls: 0.2623  loss_box_reg: 0.4312  loss_mask: 0.2052  loss_rpn_cls: 0.02857  loss_rpn_loc: 0.02948  time: 0.2219  data_time: 0.0048  lr: 0.0043956  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:48 d2.utils.events]: \u001b[0m eta: 0:15:10  iter: 899  total_loss: 1.066  loss_cls: 0.3087  loss_box_reg: 0.4874  loss_mask: 0.2032  loss_rpn_cls: 0.08323  loss_rpn_loc: 0.06055  time: 0.2219  data_time: 0.0049  lr: 0.0044955  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:53 d2.utils.events]: \u001b[0m eta: 0:15:05  iter: 919  total_loss: 1.009  loss_cls: 0.2298  loss_box_reg: 0.4816  loss_mask: 0.1882  loss_rpn_cls: 0.04283  loss_rpn_loc: 0.04016  time: 0.2220  data_time: 0.0048  lr: 0.0045954  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:51:58 d2.utils.events]: \u001b[0m eta: 0:15:01  iter: 939  total_loss: 1.099  loss_cls: 0.2741  loss_box_reg: 0.4968  loss_mask: 0.1775  loss_rpn_cls: 0.05787  loss_rpn_loc: 0.04791  time: 0.2221  data_time: 0.0049  lr: 0.0046953  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:02 d2.utils.events]: \u001b[0m eta: 0:14:57  iter: 959  total_loss: 0.8867  loss_cls: 0.2167  loss_box_reg: 0.3413  loss_mask: 0.1771  loss_rpn_cls: 0.0654  loss_rpn_loc: 0.03991  time: 0.2221  data_time: 0.0047  lr: 0.0047952  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:07 d2.utils.events]: \u001b[0m eta: 0:14:53  iter: 979  total_loss: 0.9684  loss_cls: 0.2133  loss_box_reg: 0.4247  loss_mask: 0.2186  loss_rpn_cls: 0.0373  loss_rpn_loc: 0.03555  time: 0.2221  data_time: 0.0047  lr: 0.0048951  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:11 d2.utils.events]: \u001b[0m eta: 0:14:49  iter: 999  total_loss: 1.061  loss_cls: 0.2426  loss_box_reg: 0.526  loss_mask: 0.1571  loss_rpn_cls: 0.04416  loss_rpn_loc: 0.05517  time: 0.2222  data_time: 0.0049  lr: 0.004995  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:16 d2.utils.events]: \u001b[0m eta: 0:14:44  iter: 1019  total_loss: 0.9497  loss_cls: 0.2241  loss_box_reg: 0.3831  loss_mask: 0.183  loss_rpn_cls: 0.08903  loss_rpn_loc: 0.05026  time: 0.2221  data_time: 0.0050  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:20 d2.utils.events]: \u001b[0m eta: 0:14:42  iter: 1039  total_loss: 0.9466  loss_cls: 0.2328  loss_box_reg: 0.4275  loss_mask: 0.1857  loss_rpn_cls: 0.03341  loss_rpn_loc: 0.05106  time: 0.2224  data_time: 0.0049  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:25 d2.utils.events]: \u001b[0m eta: 0:14:39  iter: 1059  total_loss: 0.9664  loss_cls: 0.2094  loss_box_reg: 0.3564  loss_mask: 0.2038  loss_rpn_cls: 0.02193  loss_rpn_loc: 0.03691  time: 0.2225  data_time: 0.0048  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:29 d2.utils.events]: \u001b[0m eta: 0:14:34  iter: 1079  total_loss: 0.8949  loss_cls: 0.1979  loss_box_reg: 0.3555  loss_mask: 0.2178  loss_rpn_cls: 0.04633  loss_rpn_loc: 0.05146  time: 0.2225  data_time: 0.0047  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:34 d2.utils.events]: \u001b[0m eta: 0:14:32  iter: 1099  total_loss: 1.04  loss_cls: 0.2729  loss_box_reg: 0.4362  loss_mask: 0.1746  loss_rpn_cls: 0.07033  loss_rpn_loc: 0.0539  time: 0.2227  data_time: 0.0051  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:39 d2.utils.events]: \u001b[0m eta: 0:14:29  iter: 1119  total_loss: 0.9266  loss_cls: 0.2158  loss_box_reg: 0.4612  loss_mask: 0.1671  loss_rpn_cls: 0.03131  loss_rpn_loc: 0.05081  time: 0.2228  data_time: 0.0049  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:43 d2.utils.events]: \u001b[0m eta: 0:14:24  iter: 1139  total_loss: 0.9416  loss_cls: 0.2304  loss_box_reg: 0.4418  loss_mask: 0.133  loss_rpn_cls: 0.04218  loss_rpn_loc: 0.05142  time: 0.2229  data_time: 0.0049  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:48 d2.utils.events]: \u001b[0m eta: 0:14:21  iter: 1159  total_loss: 0.9911  loss_cls: 0.2206  loss_box_reg: 0.4836  loss_mask: 0.1834  loss_rpn_cls: 0.02125  loss_rpn_loc: 0.04283  time: 0.2231  data_time: 0.0048  lr: 0.005  max_mem: 2118M\n",
      "\u001b[32m[07/02 23:52:53 d2.utils.events]: \u001b[0m eta: 0:14:17  iter: 1179  total_loss: 1.189  loss_cls: 0.2635  loss_box_reg: 0.4894  loss_mask: 0.2247  loss_rpn_cls: 0.06377  loss_rpn_loc: 0.06998  time: 0.2231  data_time: 0.0050  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:52:57 d2.utils.events]: \u001b[0m eta: 0:14:12  iter: 1199  total_loss: 0.8968  loss_cls: 0.2172  loss_box_reg: 0.3791  loss_mask: 0.1923  loss_rpn_cls: 0.02047  loss_rpn_loc: 0.04398  time: 0.2231  data_time: 0.0048  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:02 d2.utils.events]: \u001b[0m eta: 0:14:09  iter: 1219  total_loss: 1.041  loss_cls: 0.2348  loss_box_reg: 0.4391  loss_mask: 0.2297  loss_rpn_cls: 0.05852  loss_rpn_loc: 0.05091  time: 0.2232  data_time: 0.0049  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:06 d2.utils.events]: \u001b[0m eta: 0:14:04  iter: 1239  total_loss: 0.7638  loss_cls: 0.2074  loss_box_reg: 0.3058  loss_mask: 0.1769  loss_rpn_cls: 0.06056  loss_rpn_loc: 0.04617  time: 0.2231  data_time: 0.0046  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:11 d2.utils.events]: \u001b[0m eta: 0:14:00  iter: 1259  total_loss: 0.8891  loss_cls: 0.2313  loss_box_reg: 0.4391  loss_mask: 0.1292  loss_rpn_cls: 0.02951  loss_rpn_loc: 0.04083  time: 0.2233  data_time: 0.0048  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:15 d2.utils.events]: \u001b[0m eta: 0:13:56  iter: 1279  total_loss: 0.9366  loss_cls: 0.209  loss_box_reg: 0.4173  loss_mask: 0.1432  loss_rpn_cls: 0.03563  loss_rpn_loc: 0.05033  time: 0.2234  data_time: 0.0048  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:20 d2.utils.events]: \u001b[0m eta: 0:13:53  iter: 1299  total_loss: 0.9804  loss_cls: 0.1951  loss_box_reg: 0.4067  loss_mask: 0.1717  loss_rpn_cls: 0.02394  loss_rpn_loc: 0.04081  time: 0.2235  data_time: 0.0048  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:24 d2.utils.events]: \u001b[0m eta: 0:13:49  iter: 1319  total_loss: 0.8077  loss_cls: 0.1641  loss_box_reg: 0.3835  loss_mask: 0.1258  loss_rpn_cls: 0.01958  loss_rpn_loc: 0.04425  time: 0.2234  data_time: 0.0046  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:29 d2.utils.events]: \u001b[0m eta: 0:13:44  iter: 1339  total_loss: 0.9354  loss_cls: 0.227  loss_box_reg: 0.4326  loss_mask: 0.1403  loss_rpn_cls: 0.03019  loss_rpn_loc: 0.05001  time: 0.2235  data_time: 0.0047  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:34 d2.utils.events]: \u001b[0m eta: 0:13:39  iter: 1359  total_loss: 0.9973  loss_cls: 0.2606  loss_box_reg: 0.4804  loss_mask: 0.1826  loss_rpn_cls: 0.02861  loss_rpn_loc: 0.03603  time: 0.2235  data_time: 0.0048  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:38 d2.utils.events]: \u001b[0m eta: 0:13:35  iter: 1379  total_loss: 0.9361  loss_cls: 0.2268  loss_box_reg: 0.4467  loss_mask: 0.1675  loss_rpn_cls: 0.03655  loss_rpn_loc: 0.03495  time: 0.2236  data_time: 0.0046  lr: 0.005  max_mem: 2122M\n",
      "\u001b[32m[07/02 23:53:43 d2.utils.events]: \u001b[0m eta: 0:13:31  iter: 1399  total_loss: 0.9121  loss_cls: 0.242  loss_box_reg: 0.382  loss_mask: 0.1627  loss_rpn_cls: 0.02631  loss_rpn_loc: 0.04253  time: 0.2236  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:53:47 d2.utils.events]: \u001b[0m eta: 0:13:26  iter: 1419  total_loss: 0.8636  loss_cls: 0.1739  loss_box_reg: 0.3642  loss_mask: 0.1233  loss_rpn_cls: 0.02599  loss_rpn_loc: 0.04388  time: 0.2236  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:53:52 d2.utils.events]: \u001b[0m eta: 0:13:22  iter: 1439  total_loss: 0.9823  loss_cls: 0.2154  loss_box_reg: 0.4513  loss_mask: 0.1559  loss_rpn_cls: 0.03505  loss_rpn_loc: 0.05662  time: 0.2237  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:53:56 d2.utils.events]: \u001b[0m eta: 0:13:17  iter: 1459  total_loss: 0.8992  loss_cls: 0.2161  loss_box_reg: 0.4546  loss_mask: 0.1346  loss_rpn_cls: 0.02067  loss_rpn_loc: 0.03834  time: 0.2238  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:01 d2.utils.events]: \u001b[0m eta: 0:13:14  iter: 1479  total_loss: 0.9628  loss_cls: 0.1847  loss_box_reg: 0.4452  loss_mask: 0.1333  loss_rpn_cls: 0.02743  loss_rpn_loc: 0.03951  time: 0.2239  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:05 d2.utils.events]: \u001b[0m eta: 0:13:09  iter: 1499  total_loss: 0.8356  loss_cls: 0.1768  loss_box_reg: 0.3819  loss_mask: 0.1429  loss_rpn_cls: 0.02182  loss_rpn_loc: 0.03276  time: 0.2238  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:10 d2.utils.events]: \u001b[0m eta: 0:13:05  iter: 1519  total_loss: 0.7658  loss_cls: 0.1728  loss_box_reg: 0.3742  loss_mask: 0.1126  loss_rpn_cls: 0.01587  loss_rpn_loc: 0.0357  time: 0.2239  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:15 d2.utils.events]: \u001b[0m eta: 0:13:00  iter: 1539  total_loss: 0.8502  loss_cls: 0.2015  loss_box_reg: 0.4532  loss_mask: 0.09623  loss_rpn_cls: 0.02747  loss_rpn_loc: 0.03445  time: 0.2239  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:19 d2.utils.events]: \u001b[0m eta: 0:12:56  iter: 1559  total_loss: 0.7694  loss_cls: 0.1776  loss_box_reg: 0.3929  loss_mask: 0.1188  loss_rpn_cls: 0.01324  loss_rpn_loc: 0.0427  time: 0.2239  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:24 d2.utils.events]: \u001b[0m eta: 0:12:51  iter: 1579  total_loss: 0.6905  loss_cls: 0.1637  loss_box_reg: 0.3388  loss_mask: 0.09845  loss_rpn_cls: 0.02726  loss_rpn_loc: 0.04462  time: 0.2240  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:28 d2.utils.events]: \u001b[0m eta: 0:12:46  iter: 1599  total_loss: 0.7556  loss_cls: 0.1595  loss_box_reg: 0.3983  loss_mask: 0.1271  loss_rpn_cls: 0.01858  loss_rpn_loc: 0.03931  time: 0.2240  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:33 d2.utils.events]: \u001b[0m eta: 0:12:42  iter: 1619  total_loss: 0.7329  loss_cls: 0.153  loss_box_reg: 0.3973  loss_mask: 0.09875  loss_rpn_cls: 0.01545  loss_rpn_loc: 0.03953  time: 0.2241  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:37 d2.utils.events]: \u001b[0m eta: 0:12:38  iter: 1639  total_loss: 0.8708  loss_cls: 0.1854  loss_box_reg: 0.4329  loss_mask: 0.09869  loss_rpn_cls: 0.0191  loss_rpn_loc: 0.03717  time: 0.2241  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:42 d2.utils.events]: \u001b[0m eta: 0:12:33  iter: 1659  total_loss: 0.7411  loss_cls: 0.1751  loss_box_reg: 0.4028  loss_mask: 0.1121  loss_rpn_cls: 0.01238  loss_rpn_loc: 0.03251  time: 0.2241  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:46 d2.utils.events]: \u001b[0m eta: 0:12:29  iter: 1679  total_loss: 0.7329  loss_cls: 0.1508  loss_box_reg: 0.3455  loss_mask: 0.1012  loss_rpn_cls: 0.01101  loss_rpn_loc: 0.04424  time: 0.2241  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:51 d2.utils.events]: \u001b[0m eta: 0:12:25  iter: 1699  total_loss: 0.7784  loss_cls: 0.1774  loss_box_reg: 0.4272  loss_mask: 0.08889  loss_rpn_cls: 0.02325  loss_rpn_loc: 0.046  time: 0.2242  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:54:56 d2.utils.events]: \u001b[0m eta: 0:12:20  iter: 1719  total_loss: 0.6629  loss_cls: 0.1393  loss_box_reg: 0.3826  loss_mask: 0.1195  loss_rpn_cls: 0.01354  loss_rpn_loc: 0.04718  time: 0.2242  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:00 d2.utils.events]: \u001b[0m eta: 0:12:16  iter: 1739  total_loss: 0.866  loss_cls: 0.191  loss_box_reg: 0.4414  loss_mask: 0.09899  loss_rpn_cls: 0.01574  loss_rpn_loc: 0.04593  time: 0.2243  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:05 d2.utils.events]: \u001b[0m eta: 0:12:12  iter: 1759  total_loss: 0.8481  loss_cls: 0.1764  loss_box_reg: 0.4199  loss_mask: 0.108  loss_rpn_cls: 0.01754  loss_rpn_loc: 0.04051  time: 0.2244  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:10 d2.utils.events]: \u001b[0m eta: 0:12:08  iter: 1779  total_loss: 0.7539  loss_cls: 0.1874  loss_box_reg: 0.3948  loss_mask: 0.09107  loss_rpn_cls: 0.01579  loss_rpn_loc: 0.03422  time: 0.2244  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:14 d2.utils.events]: \u001b[0m eta: 0:12:04  iter: 1799  total_loss: 0.7482  loss_cls: 0.1772  loss_box_reg: 0.3663  loss_mask: 0.09723  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.03181  time: 0.2244  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:19 d2.utils.events]: \u001b[0m eta: 0:11:59  iter: 1819  total_loss: 0.6496  loss_cls: 0.1541  loss_box_reg: 0.3403  loss_mask: 0.09628  loss_rpn_cls: 0.02833  loss_rpn_loc: 0.04184  time: 0.2244  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:23 d2.utils.events]: \u001b[0m eta: 0:11:54  iter: 1839  total_loss: 0.7248  loss_cls: 0.1546  loss_box_reg: 0.3889  loss_mask: 0.1088  loss_rpn_cls: 0.02263  loss_rpn_loc: 0.03943  time: 0.2244  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:28 d2.utils.events]: \u001b[0m eta: 0:11:49  iter: 1859  total_loss: 0.6821  loss_cls: 0.1301  loss_box_reg: 0.3581  loss_mask: 0.09912  loss_rpn_cls: 0.0186  loss_rpn_loc: 0.03752  time: 0.2244  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:32 d2.utils.events]: \u001b[0m eta: 0:11:45  iter: 1879  total_loss: 0.6134  loss_cls: 0.1222  loss_box_reg: 0.3582  loss_mask: 0.09731  loss_rpn_cls: 0.01445  loss_rpn_loc: 0.03103  time: 0.2244  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:37 d2.utils.events]: \u001b[0m eta: 0:11:40  iter: 1899  total_loss: 0.6652  loss_cls: 0.1716  loss_box_reg: 0.3779  loss_mask: 0.09824  loss_rpn_cls: 0.0138  loss_rpn_loc: 0.03189  time: 0.2244  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:41 d2.utils.events]: \u001b[0m eta: 0:11:36  iter: 1919  total_loss: 0.5628  loss_cls: 0.1059  loss_box_reg: 0.3406  loss_mask: 0.0856  loss_rpn_cls: 0.006993  loss_rpn_loc: 0.02809  time: 0.2245  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:46 d2.utils.events]: \u001b[0m eta: 0:11:31  iter: 1939  total_loss: 0.8315  loss_cls: 0.1567  loss_box_reg: 0.4096  loss_mask: 0.09727  loss_rpn_cls: 0.02172  loss_rpn_loc: 0.04872  time: 0.2245  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:50 d2.utils.events]: \u001b[0m eta: 0:11:27  iter: 1959  total_loss: 0.6632  loss_cls: 0.1609  loss_box_reg: 0.3163  loss_mask: 0.09898  loss_rpn_cls: 0.009472  loss_rpn_loc: 0.02593  time: 0.2245  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:55:55 d2.utils.events]: \u001b[0m eta: 0:11:23  iter: 1979  total_loss: 0.7051  loss_cls: 0.1429  loss_box_reg: 0.3785  loss_mask: 0.0989  loss_rpn_cls: 0.01078  loss_rpn_loc: 0.03116  time: 0.2246  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:00 d2.utils.events]: \u001b[0m eta: 0:11:18  iter: 1999  total_loss: 0.7146  loss_cls: 0.1452  loss_box_reg: 0.3781  loss_mask: 0.09109  loss_rpn_cls: 0.01311  loss_rpn_loc: 0.05071  time: 0.2246  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:04 d2.utils.events]: \u001b[0m eta: 0:11:13  iter: 2019  total_loss: 0.6069  loss_cls: 0.128  loss_box_reg: 0.3561  loss_mask: 0.08688  loss_rpn_cls: 0.01195  loss_rpn_loc: 0.033  time: 0.2246  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:09 d2.utils.events]: \u001b[0m eta: 0:11:09  iter: 2039  total_loss: 0.5795  loss_cls: 0.1244  loss_box_reg: 0.3413  loss_mask: 0.09299  loss_rpn_cls: 0.008702  loss_rpn_loc: 0.02988  time: 0.2246  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:14 d2.utils.events]: \u001b[0m eta: 0:11:04  iter: 2059  total_loss: 0.7845  loss_cls: 0.1531  loss_box_reg: 0.3712  loss_mask: 0.08944  loss_rpn_cls: 0.01515  loss_rpn_loc: 0.04947  time: 0.2247  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:18 d2.utils.events]: \u001b[0m eta: 0:11:00  iter: 2079  total_loss: 0.6994  loss_cls: 0.1522  loss_box_reg: 0.4013  loss_mask: 0.08617  loss_rpn_cls: 0.01047  loss_rpn_loc: 0.03346  time: 0.2247  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:22 d2.utils.events]: \u001b[0m eta: 0:10:55  iter: 2099  total_loss: 0.5492  loss_cls: 0.1205  loss_box_reg: 0.3024  loss_mask: 0.07819  loss_rpn_cls: 0.01104  loss_rpn_loc: 0.03374  time: 0.2247  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:27 d2.utils.events]: \u001b[0m eta: 0:10:50  iter: 2119  total_loss: 0.581  loss_cls: 0.1064  loss_box_reg: 0.3141  loss_mask: 0.09154  loss_rpn_cls: 0.005809  loss_rpn_loc: 0.03305  time: 0.2247  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:32 d2.utils.events]: \u001b[0m eta: 0:10:46  iter: 2139  total_loss: 0.6718  loss_cls: 0.1237  loss_box_reg: 0.3754  loss_mask: 0.08975  loss_rpn_cls: 0.02216  loss_rpn_loc: 0.03886  time: 0.2248  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:36 d2.utils.events]: \u001b[0m eta: 0:10:41  iter: 2159  total_loss: 0.7104  loss_cls: 0.1608  loss_box_reg: 0.4003  loss_mask: 0.09673  loss_rpn_cls: 0.01855  loss_rpn_loc: 0.04404  time: 0.2248  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:41 d2.utils.events]: \u001b[0m eta: 0:10:37  iter: 2179  total_loss: 0.6252  loss_cls: 0.09616  loss_box_reg: 0.3076  loss_mask: 0.1234  loss_rpn_cls: 0.01272  loss_rpn_loc: 0.02776  time: 0.2248  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:45 d2.utils.events]: \u001b[0m eta: 0:10:32  iter: 2199  total_loss: 0.5522  loss_cls: 0.1221  loss_box_reg: 0.3102  loss_mask: 0.08508  loss_rpn_cls: 0.01073  loss_rpn_loc: 0.033  time: 0.2248  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:50 d2.utils.events]: \u001b[0m eta: 0:10:27  iter: 2219  total_loss: 0.6117  loss_cls: 0.1304  loss_box_reg: 0.3549  loss_mask: 0.09431  loss_rpn_cls: 0.008571  loss_rpn_loc: 0.03595  time: 0.2248  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:54 d2.utils.events]: \u001b[0m eta: 0:10:23  iter: 2239  total_loss: 0.7494  loss_cls: 0.1467  loss_box_reg: 0.3933  loss_mask: 0.102  loss_rpn_cls: 0.0163  loss_rpn_loc: 0.04158  time: 0.2248  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:56:59 d2.utils.events]: \u001b[0m eta: 0:10:19  iter: 2259  total_loss: 0.7483  loss_cls: 0.1729  loss_box_reg: 0.4041  loss_mask: 0.07658  loss_rpn_cls: 0.01375  loss_rpn_loc: 0.0361  time: 0.2249  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:04 d2.utils.events]: \u001b[0m eta: 0:10:14  iter: 2279  total_loss: 0.616  loss_cls: 0.1051  loss_box_reg: 0.3555  loss_mask: 0.08952  loss_rpn_cls: 0.01364  loss_rpn_loc: 0.03459  time: 0.2249  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:08 d2.utils.events]: \u001b[0m eta: 0:10:09  iter: 2299  total_loss: 0.5209  loss_cls: 0.104  loss_box_reg: 0.3107  loss_mask: 0.09038  loss_rpn_cls: 0.01022  loss_rpn_loc: 0.03667  time: 0.2249  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:13 d2.utils.events]: \u001b[0m eta: 0:10:05  iter: 2319  total_loss: 0.6399  loss_cls: 0.1085  loss_box_reg: 0.3851  loss_mask: 0.08004  loss_rpn_cls: 0.01162  loss_rpn_loc: 0.04236  time: 0.2249  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:17 d2.utils.events]: \u001b[0m eta: 0:10:02  iter: 2339  total_loss: 0.5891  loss_cls: 0.1499  loss_box_reg: 0.3057  loss_mask: 0.0746  loss_rpn_cls: 0.0165  loss_rpn_loc: 0.03136  time: 0.2250  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:22 d2.utils.events]: \u001b[0m eta: 0:09:56  iter: 2359  total_loss: 0.639  loss_cls: 0.1197  loss_box_reg: 0.3292  loss_mask: 0.08055  loss_rpn_cls: 0.01093  loss_rpn_loc: 0.03583  time: 0.2250  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:26 d2.utils.events]: \u001b[0m eta: 0:09:52  iter: 2379  total_loss: 0.5458  loss_cls: 0.1046  loss_box_reg: 0.3291  loss_mask: 0.06867  loss_rpn_cls: 0.008012  loss_rpn_loc: 0.0285  time: 0.2249  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:31 d2.utils.events]: \u001b[0m eta: 0:09:47  iter: 2399  total_loss: 0.5262  loss_cls: 0.1201  loss_box_reg: 0.2994  loss_mask: 0.08069  loss_rpn_cls: 0.007892  loss_rpn_loc: 0.02256  time: 0.2250  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:36 d2.utils.events]: \u001b[0m eta: 0:09:43  iter: 2419  total_loss: 0.5702  loss_cls: 0.1244  loss_box_reg: 0.3353  loss_mask: 0.06683  loss_rpn_cls: 0.007688  loss_rpn_loc: 0.03216  time: 0.2250  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:40 d2.utils.events]: \u001b[0m eta: 0:09:38  iter: 2439  total_loss: 0.4921  loss_cls: 0.09017  loss_box_reg: 0.3101  loss_mask: 0.08195  loss_rpn_cls: 0.009432  loss_rpn_loc: 0.02938  time: 0.2250  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:45 d2.utils.events]: \u001b[0m eta: 0:09:33  iter: 2459  total_loss: 0.5797  loss_cls: 0.1078  loss_box_reg: 0.3258  loss_mask: 0.06686  loss_rpn_cls: 0.01366  loss_rpn_loc: 0.04588  time: 0.2250  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:49 d2.utils.events]: \u001b[0m eta: 0:09:29  iter: 2479  total_loss: 0.6488  loss_cls: 0.1625  loss_box_reg: 0.314  loss_mask: 0.06823  loss_rpn_cls: 0.01671  loss_rpn_loc: 0.03522  time: 0.2250  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:54 d2.utils.events]: \u001b[0m eta: 0:09:24  iter: 2499  total_loss: 0.5057  loss_cls: 0.09303  loss_box_reg: 0.2948  loss_mask: 0.08607  loss_rpn_cls: 0.007382  loss_rpn_loc: 0.03731  time: 0.2250  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:57:58 d2.utils.events]: \u001b[0m eta: 0:09:20  iter: 2519  total_loss: 0.6379  loss_cls: 0.1259  loss_box_reg: 0.3223  loss_mask: 0.08128  loss_rpn_cls: 0.009965  loss_rpn_loc: 0.03315  time: 0.2250  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:03 d2.utils.events]: \u001b[0m eta: 0:09:16  iter: 2539  total_loss: 0.6028  loss_cls: 0.08294  loss_box_reg: 0.3022  loss_mask: 0.1038  loss_rpn_cls: 0.01211  loss_rpn_loc: 0.03456  time: 0.2251  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:08 d2.utils.events]: \u001b[0m eta: 0:09:12  iter: 2559  total_loss: 0.6474  loss_cls: 0.1187  loss_box_reg: 0.3207  loss_mask: 0.1095  loss_rpn_cls: 0.01599  loss_rpn_loc: 0.03844  time: 0.2251  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:12 d2.utils.events]: \u001b[0m eta: 0:09:07  iter: 2579  total_loss: 0.5504  loss_cls: 0.1073  loss_box_reg: 0.3181  loss_mask: 0.108  loss_rpn_cls: 0.01182  loss_rpn_loc: 0.03188  time: 0.2252  data_time: 0.0045  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:17 d2.utils.events]: \u001b[0m eta: 0:09:03  iter: 2599  total_loss: 0.5403  loss_cls: 0.0996  loss_box_reg: 0.2953  loss_mask: 0.08601  loss_rpn_cls: 0.007783  loss_rpn_loc: 0.02949  time: 0.2252  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:22 d2.utils.events]: \u001b[0m eta: 0:08:58  iter: 2619  total_loss: 0.6662  loss_cls: 0.1572  loss_box_reg: 0.3253  loss_mask: 0.08801  loss_rpn_cls: 0.01401  loss_rpn_loc: 0.0409  time: 0.2252  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:26 d2.utils.events]: \u001b[0m eta: 0:08:53  iter: 2639  total_loss: 0.5973  loss_cls: 0.134  loss_box_reg: 0.3372  loss_mask: 0.06747  loss_rpn_cls: 0.01099  loss_rpn_loc: 0.03794  time: 0.2252  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:31 d2.utils.events]: \u001b[0m eta: 0:08:49  iter: 2659  total_loss: 0.4746  loss_cls: 0.08942  loss_box_reg: 0.2925  loss_mask: 0.07352  loss_rpn_cls: 0.005769  loss_rpn_loc: 0.02666  time: 0.2252  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:35 d2.utils.events]: \u001b[0m eta: 0:08:44  iter: 2679  total_loss: 0.6622  loss_cls: 0.1287  loss_box_reg: 0.348  loss_mask: 0.08524  loss_rpn_cls: 0.01232  loss_rpn_loc: 0.04243  time: 0.2252  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:40 d2.utils.events]: \u001b[0m eta: 0:08:40  iter: 2699  total_loss: 0.4641  loss_cls: 0.1162  loss_box_reg: 0.268  loss_mask: 0.08061  loss_rpn_cls: 0.006203  loss_rpn_loc: 0.02477  time: 0.2252  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:44 d2.utils.events]: \u001b[0m eta: 0:08:35  iter: 2719  total_loss: 0.5742  loss_cls: 0.1124  loss_box_reg: 0.3184  loss_mask: 0.0791  loss_rpn_cls: 0.01082  loss_rpn_loc: 0.03395  time: 0.2253  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:49 d2.utils.events]: \u001b[0m eta: 0:08:30  iter: 2739  total_loss: 0.5427  loss_cls: 0.1249  loss_box_reg: 0.302  loss_mask: 0.08359  loss_rpn_cls: 0.01005  loss_rpn_loc: 0.03066  time: 0.2253  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:54 d2.utils.events]: \u001b[0m eta: 0:08:26  iter: 2759  total_loss: 0.5337  loss_cls: 0.09205  loss_box_reg: 0.3175  loss_mask: 0.07526  loss_rpn_cls: 0.009008  loss_rpn_loc: 0.04441  time: 0.2253  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:58:58 d2.utils.events]: \u001b[0m eta: 0:08:21  iter: 2779  total_loss: 0.5991  loss_cls: 0.1283  loss_box_reg: 0.3436  loss_mask: 0.07097  loss_rpn_cls: 0.009379  loss_rpn_loc: 0.03679  time: 0.2254  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:03 d2.utils.events]: \u001b[0m eta: 0:08:17  iter: 2799  total_loss: 0.4806  loss_cls: 0.09424  loss_box_reg: 0.2631  loss_mask: 0.06838  loss_rpn_cls: 0.009174  loss_rpn_loc: 0.03339  time: 0.2254  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:08 d2.utils.events]: \u001b[0m eta: 0:08:13  iter: 2819  total_loss: 0.4962  loss_cls: 0.08559  loss_box_reg: 0.296  loss_mask: 0.06888  loss_rpn_cls: 0.01182  loss_rpn_loc: 0.03068  time: 0.2254  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:12 d2.utils.events]: \u001b[0m eta: 0:08:09  iter: 2839  total_loss: 0.5928  loss_cls: 0.1175  loss_box_reg: 0.3383  loss_mask: 0.07112  loss_rpn_cls: 0.009513  loss_rpn_loc: 0.03995  time: 0.2255  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:17 d2.utils.events]: \u001b[0m eta: 0:08:05  iter: 2859  total_loss: 0.4863  loss_cls: 0.08733  loss_box_reg: 0.2688  loss_mask: 0.07617  loss_rpn_cls: 0.005541  loss_rpn_loc: 0.03423  time: 0.2255  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:22 d2.utils.events]: \u001b[0m eta: 0:08:01  iter: 2879  total_loss: 0.5041  loss_cls: 0.09621  loss_box_reg: 0.2923  loss_mask: 0.08252  loss_rpn_cls: 0.01182  loss_rpn_loc: 0.03958  time: 0.2256  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:26 d2.utils.events]: \u001b[0m eta: 0:07:57  iter: 2899  total_loss: 0.5072  loss_cls: 0.1092  loss_box_reg: 0.281  loss_mask: 0.06518  loss_rpn_cls: 0.0121  loss_rpn_loc: 0.03783  time: 0.2256  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:31 d2.utils.events]: \u001b[0m eta: 0:07:52  iter: 2919  total_loss: 0.5044  loss_cls: 0.08952  loss_box_reg: 0.2845  loss_mask: 0.06062  loss_rpn_cls: 0.008279  loss_rpn_loc: 0.03323  time: 0.2256  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:35 d2.utils.events]: \u001b[0m eta: 0:07:47  iter: 2939  total_loss: 0.519  loss_cls: 0.08515  loss_box_reg: 0.3254  loss_mask: 0.07176  loss_rpn_cls: 0.005245  loss_rpn_loc: 0.02959  time: 0.2256  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:40 d2.utils.events]: \u001b[0m eta: 0:07:43  iter: 2959  total_loss: 0.4547  loss_cls: 0.07098  loss_box_reg: 0.2776  loss_mask: 0.06326  loss_rpn_cls: 0.005258  loss_rpn_loc: 0.03088  time: 0.2256  data_time: 0.0045  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:45 d2.utils.events]: \u001b[0m eta: 0:07:38  iter: 2979  total_loss: 0.4195  loss_cls: 0.06738  loss_box_reg: 0.2551  loss_mask: 0.07095  loss_rpn_cls: 0.004577  loss_rpn_loc: 0.0259  time: 0.2257  data_time: 0.0045  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:49 d2.utils.events]: \u001b[0m eta: 0:07:34  iter: 2999  total_loss: 0.5343  loss_cls: 0.09956  loss_box_reg: 0.3003  loss_mask: 0.06267  loss_rpn_cls: 0.007808  loss_rpn_loc: 0.0414  time: 0.2257  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:54 d2.utils.events]: \u001b[0m eta: 0:07:30  iter: 3019  total_loss: 0.4323  loss_cls: 0.06749  loss_box_reg: 0.2666  loss_mask: 0.05488  loss_rpn_cls: 0.005122  loss_rpn_loc: 0.02274  time: 0.2258  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/02 23:59:59 d2.utils.events]: \u001b[0m eta: 0:07:26  iter: 3039  total_loss: 0.5669  loss_cls: 0.1067  loss_box_reg: 0.2963  loss_mask: 0.05631  loss_rpn_cls: 0.006131  loss_rpn_loc: 0.04235  time: 0.2258  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:03 d2.utils.events]: \u001b[0m eta: 0:07:21  iter: 3059  total_loss: 0.4071  loss_cls: 0.0785  loss_box_reg: 0.2578  loss_mask: 0.06331  loss_rpn_cls: 0.005621  loss_rpn_loc: 0.02926  time: 0.2258  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:08 d2.utils.events]: \u001b[0m eta: 0:07:16  iter: 3079  total_loss: 0.4744  loss_cls: 0.09279  loss_box_reg: 0.2739  loss_mask: 0.05678  loss_rpn_cls: 0.01099  loss_rpn_loc: 0.03829  time: 0.2259  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:13 d2.utils.events]: \u001b[0m eta: 0:07:13  iter: 3099  total_loss: 0.4961  loss_cls: 0.09135  loss_box_reg: 0.2612  loss_mask: 0.06637  loss_rpn_cls: 0.008605  loss_rpn_loc: 0.0313  time: 0.2260  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:17 d2.utils.events]: \u001b[0m eta: 0:07:08  iter: 3119  total_loss: 0.451  loss_cls: 0.06867  loss_box_reg: 0.2673  loss_mask: 0.0555  loss_rpn_cls: 0.006294  loss_rpn_loc: 0.03069  time: 0.2260  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:22 d2.utils.events]: \u001b[0m eta: 0:07:03  iter: 3139  total_loss: 0.4856  loss_cls: 0.08477  loss_box_reg: 0.3019  loss_mask: 0.0579  loss_rpn_cls: 0.00692  loss_rpn_loc: 0.02928  time: 0.2260  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:27 d2.utils.events]: \u001b[0m eta: 0:06:59  iter: 3159  total_loss: 0.4509  loss_cls: 0.07782  loss_box_reg: 0.2578  loss_mask: 0.06703  loss_rpn_cls: 0.00526  loss_rpn_loc: 0.02802  time: 0.2260  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:31 d2.utils.events]: \u001b[0m eta: 0:06:54  iter: 3179  total_loss: 0.4417  loss_cls: 0.08538  loss_box_reg: 0.2636  loss_mask: 0.06004  loss_rpn_cls: 0.005974  loss_rpn_loc: 0.02862  time: 0.2260  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:36 d2.utils.events]: \u001b[0m eta: 0:06:50  iter: 3199  total_loss: 0.5104  loss_cls: 0.1306  loss_box_reg: 0.2846  loss_mask: 0.06904  loss_rpn_cls: 0.00654  loss_rpn_loc: 0.03584  time: 0.2260  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:40 d2.utils.events]: \u001b[0m eta: 0:06:46  iter: 3219  total_loss: 0.4477  loss_cls: 0.07814  loss_box_reg: 0.2553  loss_mask: 0.06906  loss_rpn_cls: 0.006771  loss_rpn_loc: 0.03989  time: 0.2261  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:45 d2.utils.events]: \u001b[0m eta: 0:06:41  iter: 3239  total_loss: 0.4621  loss_cls: 0.1044  loss_box_reg: 0.2965  loss_mask: 0.05194  loss_rpn_cls: 0.004854  loss_rpn_loc: 0.03277  time: 0.2261  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:50 d2.utils.events]: \u001b[0m eta: 0:06:37  iter: 3259  total_loss: 0.4545  loss_cls: 0.09314  loss_box_reg: 0.2977  loss_mask: 0.05418  loss_rpn_cls: 0.005593  loss_rpn_loc: 0.02916  time: 0.2262  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:55 d2.utils.events]: \u001b[0m eta: 0:06:33  iter: 3279  total_loss: 0.4382  loss_cls: 0.08195  loss_box_reg: 0.2487  loss_mask: 0.06359  loss_rpn_cls: 0.004942  loss_rpn_loc: 0.02999  time: 0.2262  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:00:59 d2.utils.events]: \u001b[0m eta: 0:06:29  iter: 3299  total_loss: 0.5121  loss_cls: 0.1045  loss_box_reg: 0.2658  loss_mask: 0.07042  loss_rpn_cls: 0.006138  loss_rpn_loc: 0.03182  time: 0.2263  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:04 d2.utils.events]: \u001b[0m eta: 0:06:24  iter: 3319  total_loss: 0.4399  loss_cls: 0.05046  loss_box_reg: 0.2679  loss_mask: 0.0568  loss_rpn_cls: 0.005596  loss_rpn_loc: 0.02926  time: 0.2263  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:09 d2.utils.events]: \u001b[0m eta: 0:06:19  iter: 3339  total_loss: 0.5214  loss_cls: 0.09162  loss_box_reg: 0.2922  loss_mask: 0.05896  loss_rpn_cls: 0.005428  loss_rpn_loc: 0.03703  time: 0.2263  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:13 d2.utils.events]: \u001b[0m eta: 0:06:15  iter: 3359  total_loss: 0.4659  loss_cls: 0.07733  loss_box_reg: 0.2856  loss_mask: 0.05153  loss_rpn_cls: 0.003899  loss_rpn_loc: 0.03166  time: 0.2263  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:18 d2.utils.events]: \u001b[0m eta: 0:06:10  iter: 3379  total_loss: 0.4498  loss_cls: 0.08144  loss_box_reg: 0.2342  loss_mask: 0.05612  loss_rpn_cls: 0.00814  loss_rpn_loc: 0.03377  time: 0.2263  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:22 d2.utils.events]: \u001b[0m eta: 0:06:06  iter: 3399  total_loss: 0.4091  loss_cls: 0.07565  loss_box_reg: 0.2468  loss_mask: 0.05633  loss_rpn_cls: 0.004855  loss_rpn_loc: 0.02336  time: 0.2263  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:27 d2.utils.events]: \u001b[0m eta: 0:06:01  iter: 3419  total_loss: 0.4582  loss_cls: 0.07567  loss_box_reg: 0.2594  loss_mask: 0.06541  loss_rpn_cls: 0.004291  loss_rpn_loc: 0.02737  time: 0.2263  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:32 d2.utils.events]: \u001b[0m eta: 0:05:57  iter: 3439  total_loss: 0.4803  loss_cls: 0.09707  loss_box_reg: 0.2765  loss_mask: 0.05927  loss_rpn_cls: 0.005391  loss_rpn_loc: 0.03536  time: 0.2263  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:36 d2.utils.events]: \u001b[0m eta: 0:05:52  iter: 3459  total_loss: 0.4617  loss_cls: 0.09476  loss_box_reg: 0.2986  loss_mask: 0.04604  loss_rpn_cls: 0.007069  loss_rpn_loc: 0.0424  time: 0.2264  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:41 d2.utils.events]: \u001b[0m eta: 0:05:48  iter: 3479  total_loss: 0.4182  loss_cls: 0.07544  loss_box_reg: 0.2584  loss_mask: 0.05922  loss_rpn_cls: 0.006765  loss_rpn_loc: 0.02883  time: 0.2264  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:45 d2.utils.events]: \u001b[0m eta: 0:05:44  iter: 3499  total_loss: 0.4205  loss_cls: 0.08749  loss_box_reg: 0.2606  loss_mask: 0.05492  loss_rpn_cls: 0.004669  loss_rpn_loc: 0.03381  time: 0.2264  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:50 d2.utils.events]: \u001b[0m eta: 0:05:39  iter: 3519  total_loss: 0.4465  loss_cls: 0.09694  loss_box_reg: 0.2671  loss_mask: 0.05513  loss_rpn_cls: 0.007085  loss_rpn_loc: 0.03023  time: 0.2264  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:55 d2.utils.events]: \u001b[0m eta: 0:05:34  iter: 3539  total_loss: 0.4419  loss_cls: 0.07104  loss_box_reg: 0.2646  loss_mask: 0.06138  loss_rpn_cls: 0.005041  loss_rpn_loc: 0.02933  time: 0.2265  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:01:59 d2.utils.events]: \u001b[0m eta: 0:05:30  iter: 3559  total_loss: 0.3996  loss_cls: 0.06552  loss_box_reg: 0.2123  loss_mask: 0.0617  loss_rpn_cls: 0.002477  loss_rpn_loc: 0.02539  time: 0.2265  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:04 d2.utils.events]: \u001b[0m eta: 0:05:25  iter: 3579  total_loss: 0.4414  loss_cls: 0.07816  loss_box_reg: 0.2488  loss_mask: 0.05032  loss_rpn_cls: 0.006572  loss_rpn_loc: 0.02364  time: 0.2265  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:09 d2.utils.events]: \u001b[0m eta: 0:05:21  iter: 3599  total_loss: 0.4536  loss_cls: 0.08348  loss_box_reg: 0.2685  loss_mask: 0.06071  loss_rpn_cls: 0.005257  loss_rpn_loc: 0.03441  time: 0.2265  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:13 d2.utils.events]: \u001b[0m eta: 0:05:16  iter: 3619  total_loss: 0.3615  loss_cls: 0.05603  loss_box_reg: 0.223  loss_mask: 0.05143  loss_rpn_cls: 0.004022  loss_rpn_loc: 0.01917  time: 0.2265  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:18 d2.utils.events]: \u001b[0m eta: 0:05:12  iter: 3639  total_loss: 0.3638  loss_cls: 0.06148  loss_box_reg: 0.2267  loss_mask: 0.05808  loss_rpn_cls: 0.003497  loss_rpn_loc: 0.02708  time: 0.2266  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:23 d2.utils.events]: \u001b[0m eta: 0:05:07  iter: 3659  total_loss: 0.4863  loss_cls: 0.1027  loss_box_reg: 0.2905  loss_mask: 0.04721  loss_rpn_cls: 0.005987  loss_rpn_loc: 0.03755  time: 0.2266  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:27 d2.utils.events]: \u001b[0m eta: 0:05:03  iter: 3679  total_loss: 0.3376  loss_cls: 0.05373  loss_box_reg: 0.1977  loss_mask: 0.05886  loss_rpn_cls: 0.005662  loss_rpn_loc: 0.02395  time: 0.2266  data_time: 0.0045  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:32 d2.utils.events]: \u001b[0m eta: 0:04:58  iter: 3699  total_loss: 0.4048  loss_cls: 0.07325  loss_box_reg: 0.2575  loss_mask: 0.04687  loss_rpn_cls: 0.005119  loss_rpn_loc: 0.02811  time: 0.2266  data_time: 0.0045  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:36 d2.utils.events]: \u001b[0m eta: 0:04:53  iter: 3719  total_loss: 0.3855  loss_cls: 0.08469  loss_box_reg: 0.2186  loss_mask: 0.04484  loss_rpn_cls: 0.004288  loss_rpn_loc: 0.0313  time: 0.2266  data_time: 0.0045  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:41 d2.utils.events]: \u001b[0m eta: 0:04:49  iter: 3739  total_loss: 0.3968  loss_cls: 0.07567  loss_box_reg: 0.253  loss_mask: 0.05795  loss_rpn_cls: 0.003597  loss_rpn_loc: 0.02606  time: 0.2267  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:46 d2.utils.events]: \u001b[0m eta: 0:04:44  iter: 3759  total_loss: 0.4602  loss_cls: 0.07126  loss_box_reg: 0.2422  loss_mask: 0.05166  loss_rpn_cls: 0.004245  loss_rpn_loc: 0.02512  time: 0.2267  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:50 d2.utils.events]: \u001b[0m eta: 0:04:40  iter: 3779  total_loss: 0.4313  loss_cls: 0.08911  loss_box_reg: 0.2655  loss_mask: 0.05674  loss_rpn_cls: 0.004574  loss_rpn_loc: 0.02588  time: 0.2267  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:02:55 d2.utils.events]: \u001b[0m eta: 0:04:35  iter: 3799  total_loss: 0.4382  loss_cls: 0.08689  loss_box_reg: 0.2445  loss_mask: 0.05084  loss_rpn_cls: 0.004444  loss_rpn_loc: 0.03561  time: 0.2267  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:00 d2.utils.events]: \u001b[0m eta: 0:04:30  iter: 3819  total_loss: 0.4147  loss_cls: 0.06707  loss_box_reg: 0.2562  loss_mask: 0.05821  loss_rpn_cls: 0.004614  loss_rpn_loc: 0.02734  time: 0.2267  data_time: 0.0044  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:04 d2.utils.events]: \u001b[0m eta: 0:04:26  iter: 3839  total_loss: 0.4435  loss_cls: 0.08115  loss_box_reg: 0.247  loss_mask: 0.05087  loss_rpn_cls: 0.005025  loss_rpn_loc: 0.02506  time: 0.2267  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:09 d2.utils.events]: \u001b[0m eta: 0:04:21  iter: 3859  total_loss: 0.393  loss_cls: 0.08255  loss_box_reg: 0.2281  loss_mask: 0.05027  loss_rpn_cls: 0.005107  loss_rpn_loc: 0.03372  time: 0.2267  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:13 d2.utils.events]: \u001b[0m eta: 0:04:16  iter: 3879  total_loss: 0.4412  loss_cls: 0.09919  loss_box_reg: 0.2804  loss_mask: 0.04805  loss_rpn_cls: 0.006551  loss_rpn_loc: 0.03899  time: 0.2268  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:18 d2.utils.events]: \u001b[0m eta: 0:04:12  iter: 3899  total_loss: 0.3639  loss_cls: 0.07015  loss_box_reg: 0.2028  loss_mask: 0.05972  loss_rpn_cls: 0.004741  loss_rpn_loc: 0.02223  time: 0.2268  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:23 d2.utils.events]: \u001b[0m eta: 0:04:07  iter: 3919  total_loss: 0.4211  loss_cls: 0.06465  loss_box_reg: 0.2305  loss_mask: 0.0513  loss_rpn_cls: 0.003825  loss_rpn_loc: 0.03327  time: 0.2268  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:28 d2.utils.events]: \u001b[0m eta: 0:04:03  iter: 3939  total_loss: 0.4002  loss_cls: 0.07489  loss_box_reg: 0.2255  loss_mask: 0.05896  loss_rpn_cls: 0.005416  loss_rpn_loc: 0.0377  time: 0.2269  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:32 d2.utils.events]: \u001b[0m eta: 0:03:59  iter: 3959  total_loss: 0.399  loss_cls: 0.07443  loss_box_reg: 0.233  loss_mask: 0.05042  loss_rpn_cls: 0.005968  loss_rpn_loc: 0.0306  time: 0.2269  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:37 d2.utils.events]: \u001b[0m eta: 0:03:54  iter: 3979  total_loss: 0.4368  loss_cls: 0.08566  loss_box_reg: 0.2477  loss_mask: 0.05808  loss_rpn_cls: 0.004252  loss_rpn_loc: 0.0243  time: 0.2269  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:42 d2.utils.events]: \u001b[0m eta: 0:03:49  iter: 3999  total_loss: 0.3742  loss_cls: 0.06935  loss_box_reg: 0.2294  loss_mask: 0.0501  loss_rpn_cls: 0.004832  loss_rpn_loc: 0.0258  time: 0.2270  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:46 d2.utils.events]: \u001b[0m eta: 0:03:45  iter: 4019  total_loss: 0.3985  loss_cls: 0.06603  loss_box_reg: 0.2566  loss_mask: 0.04226  loss_rpn_cls: 0.005093  loss_rpn_loc: 0.02698  time: 0.2270  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:51 d2.utils.events]: \u001b[0m eta: 0:03:40  iter: 4039  total_loss: 0.3463  loss_cls: 0.06405  loss_box_reg: 0.2185  loss_mask: 0.05117  loss_rpn_cls: 0.002696  loss_rpn_loc: 0.02011  time: 0.2270  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:03:56 d2.utils.events]: \u001b[0m eta: 0:03:36  iter: 4059  total_loss: 0.3734  loss_cls: 0.06991  loss_box_reg: 0.2113  loss_mask: 0.05211  loss_rpn_cls: 0.003894  loss_rpn_loc: 0.02484  time: 0.2270  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:00 d2.utils.events]: \u001b[0m eta: 0:03:31  iter: 4079  total_loss: 0.3768  loss_cls: 0.06634  loss_box_reg: 0.2456  loss_mask: 0.0527  loss_rpn_cls: 0.003203  loss_rpn_loc: 0.0233  time: 0.2271  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:05 d2.utils.events]: \u001b[0m eta: 0:03:26  iter: 4099  total_loss: 0.4131  loss_cls: 0.08637  loss_box_reg: 0.2227  loss_mask: 0.04398  loss_rpn_cls: 0.006732  loss_rpn_loc: 0.03102  time: 0.2271  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:10 d2.utils.events]: \u001b[0m eta: 0:03:22  iter: 4119  total_loss: 0.3732  loss_cls: 0.06364  loss_box_reg: 0.1993  loss_mask: 0.04706  loss_rpn_cls: 0.004675  loss_rpn_loc: 0.0348  time: 0.2271  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:14 d2.utils.events]: \u001b[0m eta: 0:03:17  iter: 4139  total_loss: 0.3707  loss_cls: 0.05282  loss_box_reg: 0.2428  loss_mask: 0.04848  loss_rpn_cls: 0.002567  loss_rpn_loc: 0.02092  time: 0.2271  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:19 d2.utils.events]: \u001b[0m eta: 0:03:12  iter: 4159  total_loss: 0.4949  loss_cls: 0.07718  loss_box_reg: 0.2341  loss_mask: 0.05553  loss_rpn_cls: 0.003481  loss_rpn_loc: 0.03637  time: 0.2271  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:23 d2.utils.events]: \u001b[0m eta: 0:03:08  iter: 4179  total_loss: 0.351  loss_cls: 0.05538  loss_box_reg: 0.1978  loss_mask: 0.05196  loss_rpn_cls: 0.003315  loss_rpn_loc: 0.02179  time: 0.2271  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:28 d2.utils.events]: \u001b[0m eta: 0:03:03  iter: 4199  total_loss: 0.4118  loss_cls: 0.06576  loss_box_reg: 0.2457  loss_mask: 0.05188  loss_rpn_cls: 0.005294  loss_rpn_loc: 0.03074  time: 0.2271  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:33 d2.utils.events]: \u001b[0m eta: 0:02:59  iter: 4219  total_loss: 0.4313  loss_cls: 0.06846  loss_box_reg: 0.2247  loss_mask: 0.05759  loss_rpn_cls: 0.004297  loss_rpn_loc: 0.03581  time: 0.2271  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:37 d2.utils.events]: \u001b[0m eta: 0:02:54  iter: 4239  total_loss: 0.3836  loss_cls: 0.0724  loss_box_reg: 0.2267  loss_mask: 0.0511  loss_rpn_cls: 0.005348  loss_rpn_loc: 0.02573  time: 0.2272  data_time: 0.0046  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:42 d2.utils.events]: \u001b[0m eta: 0:02:49  iter: 4259  total_loss: 0.3768  loss_cls: 0.07184  loss_box_reg: 0.2139  loss_mask: 0.05531  loss_rpn_cls: 0.00489  loss_rpn_loc: 0.03288  time: 0.2272  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:47 d2.utils.events]: \u001b[0m eta: 0:02:45  iter: 4279  total_loss: 0.4316  loss_cls: 0.07009  loss_box_reg: 0.2711  loss_mask: 0.04643  loss_rpn_cls: 0.003927  loss_rpn_loc: 0.03231  time: 0.2272  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:51 d2.utils.events]: \u001b[0m eta: 0:02:40  iter: 4299  total_loss: 0.3547  loss_cls: 0.06191  loss_box_reg: 0.2254  loss_mask: 0.04651  loss_rpn_cls: 0.004334  loss_rpn_loc: 0.02549  time: 0.2272  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:04:56 d2.utils.events]: \u001b[0m eta: 0:02:36  iter: 4319  total_loss: 0.4198  loss_cls: 0.07346  loss_box_reg: 0.2331  loss_mask: 0.04968  loss_rpn_cls: 0.004554  loss_rpn_loc: 0.02718  time: 0.2273  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:05:01 d2.utils.events]: \u001b[0m eta: 0:02:31  iter: 4339  total_loss: 0.3966  loss_cls: 0.06891  loss_box_reg: 0.24  loss_mask: 0.03959  loss_rpn_cls: 0.003378  loss_rpn_loc: 0.02855  time: 0.2273  data_time: 0.0050  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:05:05 d2.utils.events]: \u001b[0m eta: 0:02:26  iter: 4359  total_loss: 0.3263  loss_cls: 0.05869  loss_box_reg: 0.2024  loss_mask: 0.0478  loss_rpn_cls: 0.004027  loss_rpn_loc: 0.02584  time: 0.2273  data_time: 0.0048  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:05:10 d2.utils.events]: \u001b[0m eta: 0:02:22  iter: 4379  total_loss: 0.3411  loss_cls: 0.06506  loss_box_reg: 0.2182  loss_mask: 0.05368  loss_rpn_cls: 0.002727  loss_rpn_loc: 0.03193  time: 0.2273  data_time: 0.0047  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:05:15 d2.utils.events]: \u001b[0m eta: 0:02:18  iter: 4399  total_loss: 0.4468  loss_cls: 0.07966  loss_box_reg: 0.2374  loss_mask: 0.04367  loss_rpn_cls: 0.004096  loss_rpn_loc: 0.02855  time: 0.2274  data_time: 0.0049  lr: 0.005  max_mem: 2125M\n",
      "\u001b[32m[07/03 00:05:20 d2.utils.events]: \u001b[0m eta: 0:02:13  iter: 4419  total_loss: 0.482  loss_cls: 0.07763  loss_box_reg: 0.2579  loss_mask: 0.05054  loss_rpn_cls: 0.005738  loss_rpn_loc: 0.04003  time: 0.2274  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:24 d2.utils.events]: \u001b[0m eta: 0:02:08  iter: 4439  total_loss: 0.3105  loss_cls: 0.05091  loss_box_reg: 0.1773  loss_mask: 0.04812  loss_rpn_cls: 0.001474  loss_rpn_loc: 0.01654  time: 0.2274  data_time: 0.0047  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:29 d2.utils.events]: \u001b[0m eta: 0:02:04  iter: 4459  total_loss: 0.3787  loss_cls: 0.05032  loss_box_reg: 0.2273  loss_mask: 0.04267  loss_rpn_cls: 0.002542  loss_rpn_loc: 0.02448  time: 0.2274  data_time: 0.0047  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:33 d2.utils.events]: \u001b[0m eta: 0:01:59  iter: 4479  total_loss: 0.368  loss_cls: 0.05404  loss_box_reg: 0.231  loss_mask: 0.04797  loss_rpn_cls: 0.00195  loss_rpn_loc: 0.02553  time: 0.2274  data_time: 0.0047  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:38 d2.utils.events]: \u001b[0m eta: 0:01:54  iter: 4499  total_loss: 0.3567  loss_cls: 0.06435  loss_box_reg: 0.1927  loss_mask: 0.04739  loss_rpn_cls: 0.002696  loss_rpn_loc: 0.02697  time: 0.2274  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:43 d2.utils.events]: \u001b[0m eta: 0:01:50  iter: 4519  total_loss: 0.4038  loss_cls: 0.06149  loss_box_reg: 0.2345  loss_mask: 0.03938  loss_rpn_cls: 0.004823  loss_rpn_loc: 0.02767  time: 0.2275  data_time: 0.0051  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:47 d2.utils.events]: \u001b[0m eta: 0:01:45  iter: 4539  total_loss: 0.3645  loss_cls: 0.05479  loss_box_reg: 0.204  loss_mask: 0.04498  loss_rpn_cls: 0.003725  loss_rpn_loc: 0.02918  time: 0.2275  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:52 d2.utils.events]: \u001b[0m eta: 0:01:41  iter: 4559  total_loss: 0.3111  loss_cls: 0.05606  loss_box_reg: 0.201  loss_mask: 0.04823  loss_rpn_cls: 0.004712  loss_rpn_loc: 0.02389  time: 0.2275  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:05:57 d2.utils.events]: \u001b[0m eta: 0:01:36  iter: 4579  total_loss: 0.3509  loss_cls: 0.06523  loss_box_reg: 0.1922  loss_mask: 0.04965  loss_rpn_cls: 0.003075  loss_rpn_loc: 0.02496  time: 0.2276  data_time: 0.0050  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:02 d2.utils.events]: \u001b[0m eta: 0:01:32  iter: 4599  total_loss: 0.3091  loss_cls: 0.05146  loss_box_reg: 0.1887  loss_mask: 0.04891  loss_rpn_cls: 0.003481  loss_rpn_loc: 0.0221  time: 0.2276  data_time: 0.0047  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:06 d2.utils.events]: \u001b[0m eta: 0:01:27  iter: 4619  total_loss: 0.3464  loss_cls: 0.06077  loss_box_reg: 0.2119  loss_mask: 0.04517  loss_rpn_cls: 0.003475  loss_rpn_loc: 0.03415  time: 0.2276  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:11 d2.utils.events]: \u001b[0m eta: 0:01:22  iter: 4639  total_loss: 0.3419  loss_cls: 0.0517  loss_box_reg: 0.1992  loss_mask: 0.05374  loss_rpn_cls: 0.003016  loss_rpn_loc: 0.01924  time: 0.2276  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:16 d2.utils.events]: \u001b[0m eta: 0:01:18  iter: 4659  total_loss: 0.3183  loss_cls: 0.05194  loss_box_reg: 0.1848  loss_mask: 0.04384  loss_rpn_cls: 0.003723  loss_rpn_loc: 0.02277  time: 0.2276  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:20 d2.utils.events]: \u001b[0m eta: 0:01:13  iter: 4679  total_loss: 0.3381  loss_cls: 0.05873  loss_box_reg: 0.2047  loss_mask: 0.03986  loss_rpn_cls: 0.002766  loss_rpn_loc: 0.02883  time: 0.2277  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:25 d2.utils.events]: \u001b[0m eta: 0:01:09  iter: 4699  total_loss: 0.3633  loss_cls: 0.0535  loss_box_reg: 0.2297  loss_mask: 0.04637  loss_rpn_cls: 0.002968  loss_rpn_loc: 0.03012  time: 0.2276  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:30 d2.utils.events]: \u001b[0m eta: 0:01:04  iter: 4719  total_loss: 0.3409  loss_cls: 0.05609  loss_box_reg: 0.2022  loss_mask: 0.04306  loss_rpn_cls: 0.003122  loss_rpn_loc: 0.02162  time: 0.2277  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:34 d2.utils.events]: \u001b[0m eta: 0:00:59  iter: 4739  total_loss: 0.3623  loss_cls: 0.06479  loss_box_reg: 0.2352  loss_mask: 0.04937  loss_rpn_cls: 0.004048  loss_rpn_loc: 0.02723  time: 0.2276  data_time: 0.0047  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:39 d2.utils.events]: \u001b[0m eta: 0:00:55  iter: 4759  total_loss: 0.4049  loss_cls: 0.07409  loss_box_reg: 0.2136  loss_mask: 0.04116  loss_rpn_cls: 0.00503  loss_rpn_loc: 0.02825  time: 0.2277  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:43 d2.utils.events]: \u001b[0m eta: 0:00:50  iter: 4779  total_loss: 0.3144  loss_cls: 0.0611  loss_box_reg: 0.2054  loss_mask: 0.04991  loss_rpn_cls: 0.001991  loss_rpn_loc: 0.02149  time: 0.2277  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:48 d2.utils.events]: \u001b[0m eta: 0:00:46  iter: 4799  total_loss: 0.3409  loss_cls: 0.06105  loss_box_reg: 0.2035  loss_mask: 0.03961  loss_rpn_cls: 0.003415  loss_rpn_loc: 0.02179  time: 0.2277  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:53 d2.utils.events]: \u001b[0m eta: 0:00:41  iter: 4819  total_loss: 0.3736  loss_cls: 0.06462  loss_box_reg: 0.2167  loss_mask: 0.04616  loss_rpn_cls: 0.002824  loss_rpn_loc: 0.02935  time: 0.2277  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:06:57 d2.utils.events]: \u001b[0m eta: 0:00:36  iter: 4839  total_loss: 0.3272  loss_cls: 0.05729  loss_box_reg: 0.1887  loss_mask: 0.05231  loss_rpn_cls: 0.001505  loss_rpn_loc: 0.02126  time: 0.2277  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:02 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 4859  total_loss: 0.3728  loss_cls: 0.07585  loss_box_reg: 0.2144  loss_mask: 0.05208  loss_rpn_cls: 0.002364  loss_rpn_loc: 0.02107  time: 0.2277  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:07 d2.utils.events]: \u001b[0m eta: 0:00:27  iter: 4879  total_loss: 0.3507  loss_cls: 0.05787  loss_box_reg: 0.2191  loss_mask: 0.03801  loss_rpn_cls: 0.003162  loss_rpn_loc: 0.02091  time: 0.2277  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:11 d2.utils.events]: \u001b[0m eta: 0:00:23  iter: 4899  total_loss: 0.3287  loss_cls: 0.05289  loss_box_reg: 0.2044  loss_mask: 0.0444  loss_rpn_cls: 0.004034  loss_rpn_loc: 0.03204  time: 0.2278  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:16 d2.utils.events]: \u001b[0m eta: 0:00:18  iter: 4919  total_loss: 0.4288  loss_cls: 0.0805  loss_box_reg: 0.2552  loss_mask: 0.03861  loss_rpn_cls: 0.004116  loss_rpn_loc: 0.03524  time: 0.2278  data_time: 0.0049  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:21 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 4939  total_loss: 0.3164  loss_cls: 0.04383  loss_box_reg: 0.1765  loss_mask: 0.04936  loss_rpn_cls: 0.002661  loss_rpn_loc: 0.02257  time: 0.2278  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:25 d2.utils.events]: \u001b[0m eta: 0:00:09  iter: 4959  total_loss: 0.2947  loss_cls: 0.04819  loss_box_reg: 0.1615  loss_mask: 0.04387  loss_rpn_cls: 0.002919  loss_rpn_loc: 0.02409  time: 0.2278  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:30 d2.utils.events]: \u001b[0m eta: 0:00:04  iter: 4979  total_loss: 0.3374  loss_cls: 0.06115  loss_box_reg: 0.1984  loss_mask: 0.04607  loss_rpn_cls: 0.002338  loss_rpn_loc: 0.02746  time: 0.2278  data_time: 0.0048  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:37 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 4999  total_loss: 0.3629  loss_cls: 0.06073  loss_box_reg: 0.2074  loss_mask: 0.04231  loss_rpn_cls: 0.003599  loss_rpn_loc: 0.02414  time: 0.2278  data_time: 0.0047  lr: 0.005  max_mem: 2126M\n",
      "\u001b[32m[07/03 00:07:38 d2.engine.hooks]: \u001b[0mOverall training speed: 4998 iterations in 0:18:58 (0.2278 s / it)\n",
      "\u001b[32m[07/03 00:07:38 d2.engine.hooks]: \u001b[0mTotal training time: 0:19:10 (0:00:11 on hooks)\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"cell_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.005  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 5000 \n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2 \n",
    "cfg.INPUT.MASK_FORMAT='bitmask'\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': Instances(num_instances=4, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  0.7397,   0.0000, 227.0000, 227.0000],\n",
      "        [203.2011,   0.0000, 226.1115,  24.7901],\n",
      "        [212.8005,   0.0000, 226.4003,  21.1937],\n",
      "        [213.2568, 168.6918, 226.1651, 186.8922]], device='cuda:0')), scores: tensor([0.9985, 0.9933, 0.8252, 0.7346], device='cuda:0'), pred_classes: tensor([0, 0, 1, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True, False],\n",
      "         [False, False, False,  ...,  True,  True, False],\n",
      "         [False, False, False,  ...,  True,  True, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=4, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  0.0000,   0.0000, 227.0000, 227.0000],\n",
      "        [  0.3493, 202.6547,  27.0043, 227.0000],\n",
      "        [ 71.5758,   0.0000, 116.3573,  13.7013],\n",
      "        [195.4517, 181.9509, 225.1884, 226.7165]], device='cuda:0')), scores: tensor([0.9986, 0.8934, 0.8551, 0.7862], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True, False, False],\n",
      "         [False, False, False,  ...,  True, False, False],\n",
      "         [False, False, False,  ...,  True, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  4.5651,   0.0000, 227.0000, 225.1705],\n",
      "        [ 65.0964,   5.2666, 209.4149, 200.9023]], device='cuda:0')), scores: tensor([0.9985, 0.9367], device='cuda:0'), pred_classes: tensor([0, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=3, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  0.0000,   0.8115, 227.0000, 224.8115],\n",
      "        [213.8925,  44.0850, 226.4505,  60.2314],\n",
      "        [ 13.4724,  16.7119, 158.5645, 121.8719]], device='cuda:0')), scores: tensor([0.9989, 0.8540, 0.7642], device='cuda:0'), pred_classes: tensor([0, 1, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  3.2192,   2.0907, 223.8017, 226.0098],\n",
      "        [ 26.6544,  36.9573, 194.8614, 204.4629]], device='cuda:0')), scores: tensor([0.9951, 0.9867], device='cuda:0'), pred_classes: tensor([0, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=1, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  0.3207,   0.0000, 223.9418, 226.4671]], device='cuda:0')), scores: tensor([0.9979], device='cuda:0'), pred_classes: tensor([0], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  0.0000,   0.0000, 227.0000, 227.0000],\n",
      "        [ 48.7044, 190.5051,  85.5507, 224.0071]], device='cuda:0')), scores: tensor([0.9982, 0.7135], device='cuda:0'), pred_classes: tensor([0, 0], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  7.6862,   1.8157, 227.0000, 220.3721],\n",
      "        [ 16.6127,  40.2662, 146.3051, 206.8934]], device='cuda:0')), scores: tensor([0.9864, 0.9550], device='cuda:0'), pred_classes: tensor([0, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  2.4802,   0.0000, 220.8849, 226.4882],\n",
      "        [ 11.1424,   0.0000, 192.6882, 210.2876]], device='cuda:0')), scores: tensor([0.9865, 0.9442], device='cuda:0'), pred_classes: tensor([0, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=227, image_width=227, fields=[pred_boxes: Boxes(tensor([[  6.3823,   0.0000, 224.4939, 226.5199],\n",
      "        [ 20.4624,  36.6236, 144.3383, 210.3190]], device='cuda:0')), scores: tensor([0.9902, 0.9366], device='cuda:0'), pred_classes: tensor([0, 1], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "dataset_dicts = get_cell_dicts(\"data/val/\")\n",
    "for d in random.sample(dataset_dicts, 10):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im) #read about the format here: https://detectron2.readthedocs.io/en/latest/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    out.save(\"images/\" + d[\"file_name\"].split(\"/\")[-1])\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=192.168.5.112:0.0\n"
     ]
    }
   ],
   "source": [
    "%env DISPLAY=192.168.5.112:0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
